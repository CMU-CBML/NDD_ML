{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/ussqww/.conda/envs/pyg/lib/python3.12/site-packages/torch_geometric/typing.py:54: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /ocean/projects/eng170006p/ussqww/.conda/envs/pyg/lib/python3.12/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/jet/home/ussqww/.conda/envs/pyg/lib/python3.12/site-packages/torch_geometric/typing.py:110: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /ocean/projects/eng170006p/ussqww/.conda/envs/pyg/lib/python3.12/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "import my_utils as myu\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "# from torch_geometric.nn import SplineConv\n",
    "from torch_geometric.typing import WITH_TORCH_SPLINE_CONV\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using devices: ['cuda:0']\n"
     ]
    }
   ],
   "source": [
    "# check for valid spline_conv\n",
    "if not WITH_TORCH_SPLINE_CONV:\n",
    "    quit(\"This example requires 'torch-spline-conv'\")\n",
    "\n",
    "# Check available GPUs and prepare device list\n",
    "if torch.cuda.is_available():\n",
    "    devices = [f'cuda:{i}' for i in range(torch.cuda.device_count())]\n",
    "else:\n",
    "    devices = ['cpu']  # fallback to CPU if no GPUs are available\n",
    "print(\"Using devices:\", devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder 1/4 (25.00%) completed: Reading from: ./dataset/NDDs/io2D_1c03/outputs\n",
      "Processing folder 2/4 (50.00%) completed: Reading from: ./dataset/NDDs/io2D_1c02/outputs\n",
      "Processing folder 3/4 (75.00%) completed: Reading from: ./dataset/NDDs/io2D_1c01/outputs\n",
      "Processing folder 4/4 (100.00%) completed: Reading from: ./dataset/NDDs/io2D_2c01/outputs\n"
     ]
    }
   ],
   "source": [
    "def sort_key_func(filename):\n",
    "    \"\"\"\n",
    "    Extracts numbers from a filename and converts them to an integer for sorting.\n",
    "    \"\"\"\n",
    "    numbers = re.findall(r'\\d+', filename)\n",
    "    return int(numbers[0]) if numbers else float('inf')\n",
    "\n",
    "def read_vtk_file_pair(folder_path):\n",
    "    \"\"\"\n",
    "    Reads the first and last VTK files in the specified folder that start with \"physics_allparticles\"\n",
    "    and end with \".vtk\", sorted by numeric order in filenames.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing VTK files.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing data from the middle and last VTK files.\n",
    "    \"\"\"\n",
    "    # Filter files that start with \"physics_allparticles\" and end with \".vtk\"\n",
    "    vtk_files = [file for file in os.listdir(folder_path) \n",
    "                 if file.startswith(\"physics_allparticle\") and file.endswith(\".vtk\")]\n",
    "    \n",
    "    vtk_files_sorted = sorted(vtk_files, key=sort_key_func)\n",
    "\n",
    "    if not vtk_files_sorted:\n",
    "        return None\n",
    "\n",
    "    # Calculate the middle index\n",
    "    middle_index = len(vtk_files_sorted) // 2\n",
    "\n",
    "    # Read the middle and the last files\n",
    "    middle_file_path = os.path.join(folder_path, vtk_files_sorted[middle_index])\n",
    "    # last_file_path = os.path.join(folder_path, vtk_files_sorted[-1])\n",
    "    last_file_path = os.path.join(folder_path, vtk_files_sorted[middle_index+2])\n",
    "\n",
    "    # Assuming `read_mesh_allCon` is defined to read VTK files\n",
    "    first_data = myu.read_mesh_allCon(middle_file_path, 3)\n",
    "    last_data = myu.read_mesh_allCon(last_file_path, 3)\n",
    "\n",
    "    return (first_data, last_data)\n",
    "\n",
    "\n",
    "def read_all_folders_vtk_pairs(root_folder):\n",
    "    \"\"\"\n",
    "    Reads the first and last VTK files from each subfolder within the root folder that start with \"io2D\".\n",
    "\n",
    "    Args:\n",
    "        root_folder (str): Path to the root folder containing subfolders with VTK files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, each containing data from the first and last VTK files from each subfolder.\n",
    "    \"\"\"\n",
    "    folders = [f for f in os.listdir(root_folder) if f.startswith(\"io2D\")]\n",
    "    # folders = folders[:7] # for testing\n",
    "    # print(folders)\n",
    "    total_folders = len(folders)\n",
    "    vtk_pairs = []\n",
    "    \n",
    "    for index, folder in enumerate(folders):\n",
    "        outputs_path = os.path.join(root_folder, folder, \"outputs\")\n",
    "        if os.path.isdir(outputs_path):\n",
    "            vtk_pair = read_vtk_file_pair(outputs_path)\n",
    "            if vtk_pair:\n",
    "                vtk_pairs.append(vtk_pair)\n",
    "\n",
    "        # Calculate and print the progress percentage\n",
    "        progress_percent = ((index + 1) / total_folders) * 100\n",
    "        print(f\"Processing folder {index + 1}/{total_folders} ({progress_percent:.2f}%) completed: Reading from: {outputs_path}\")\n",
    "\n",
    "    return vtk_pairs\n",
    "\n",
    "# root_folder = \"/ocean/projects/eng170006p/ussqww/TTNG_05052024\"  \n",
    "# root_folder = \"E:/NeuronGrowth_2024/TTNG_05052024\"  \n",
    "root_folder = \"./dataset/NDDs\" \n",
    "vtk_pairs = read_all_folders_vtk_pairs(root_folder)\n",
    "for pair in vtk_pairs:\n",
    "    first_data, last_data = pair\n",
    "    # print(\"First File Data:\", first_data)\n",
    "    # print(\"Last File Data:\", last_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pair 1 of 4 (25.00%) completed\n",
      "Processing pair 2 of 4 (50.00%) completed\n",
      "Processing pair 3 of 4 (75.00%) completed\n",
      "Processing pair 4 of 4 (100.00%) completed\n"
     ]
    }
   ],
   "source": [
    "def calculate_pseudo_coordinates(points, edge_index):\n",
    "    \"\"\"Calculate pseudo-coordinates for each edge based on node coordinates.\"\"\"\n",
    "    pseudo_coords = []\n",
    "    for src, dest in edge_index.t().tolist():\n",
    "        delta_x = points[dest, 0] - points[src, 0]\n",
    "        delta_y = points[dest, 1] - points[src, 1]\n",
    "        pseudo_coords.append([delta_x, delta_y])\n",
    "\n",
    "    pseudo_coords = torch.tensor(pseudo_coords, dtype=torch.float)\n",
    "    return pseudo_coords\n",
    "\n",
    "def interpolate_features(current_points, current_point_data, next_points, k=3):\n",
    "    \"\"\"\n",
    "    Interpolate feature data from current points to next points using k-nearest neighbors.\n",
    "\n",
    "    Args:\n",
    "        current_points (array): Coordinates of current points where data is known.\n",
    "        current_point_data (dict): Dictionary mapping feature names to arrays of values.\n",
    "        next_points (array): Coordinates of next points where data needs to be interpolated.\n",
    "        k (int): Number of nearest neighbors to consider for interpolation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with interpolated features for each point in next_points.\n",
    "    \"\"\"\n",
    "    # Create KDTree from current points\n",
    "    tree = KDTree(current_points)\n",
    "    interpolated_data = {key: np.zeros(len(next_points)) for key in current_point_data.keys()}\n",
    "\n",
    "    # Iterate over each next point and interpolate features from nearest current points\n",
    "    for i, point in enumerate(next_points):\n",
    "        dists, indices = tree.query(point, k=k)  # Find k nearest neighbors\n",
    "        \n",
    "        # Handle cases where less than k points are available\n",
    "        if not isinstance(indices, np.ndarray):\n",
    "            indices = [indices]\n",
    "            dists = [dists]\n",
    "\n",
    "        weights = 1 / np.maximum(dists, 1e-6)  # Calculate weights inversely proportional to distance\n",
    "        weight_sum = np.sum(weights)\n",
    "        \n",
    "        # Calculate weighted average for each feature\n",
    "        for key in current_point_data.keys():\n",
    "            # Extract the specific feature values for the nearest points\n",
    "            feature_values = current_point_data[key][indices]\n",
    "            # Compute the weighted average of the feature\n",
    "            interpolated_data[key][i] = np.dot(weights, feature_values) / weight_sum\n",
    "\n",
    "    return interpolated_data\n",
    "\n",
    "def create_graph_data(points, point_data, edge_attributes, y_values):\n",
    "    \"\"\"\n",
    "    Create graph data from provided points, elements, point data, edge attributes, and target labels.\n",
    "\n",
    "    Args:\n",
    "        points (array): Coordinates of points.\n",
    "        point_data (dict): Features for each point, expected to be a dict of arrays.\n",
    "        edge_attributes (list or DataFrame): Attributes for edges.\n",
    "        y_values (array): Target labels for each point.\n",
    "    \"\"\"\n",
    "    # Convert points to tensor and ensure type is float\n",
    "    points_tensor = torch.tensor(points, dtype=torch.float)\n",
    "\n",
    "    # Prepare features tensor by concatenating feature arrays stored in point_data dictionary\n",
    "    feature_tensors = [torch.tensor(point_data[key], dtype=torch.float).unsqueeze(1) for key in point_data.keys()]\n",
    "    point_features = torch.cat([points_tensor] + feature_tensors, dim=1)\n",
    "\n",
    "    # Prepare targets using and next_phi\n",
    "    y = torch.tensor(y_values, dtype=torch.float)  # Assuming y_values are directly passable to tensor creation\n",
    "\n",
    "    # Convert edge attributes to tensor\n",
    "    if isinstance(edge_attributes, list):\n",
    "        edge_attributes = pd.DataFrame(edge_attributes)\n",
    "    edge_index = torch.tensor(edge_attributes[['node1', 'node2']].to_numpy().T, dtype=torch.long)\n",
    "\n",
    "    # Calculate pseudo-coordinates for edge attributes if needed\n",
    "    pseudo_coords = calculate_pseudo_coordinates(points_tensor, edge_index)\n",
    "    \n",
    "    # Construct graph data object\n",
    "    data = Data(x=point_features, edge_index=edge_index, edge_attr=pseudo_coords, y=y)\n",
    "    return data\n",
    "\n",
    "def create_graphs_from_datasets(vtk_pairs):\n",
    "    \"\"\"\n",
    "    Processes pairs of VTK data, where each pair's first item is current data and the second item is next data.\n",
    "\n",
    "    Args:\n",
    "        vtk_pairs (list of tuples): List where each tuple contains two sets of data (current and next).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of graph data objects, each representing processed graph data from the pairs.\n",
    "    \"\"\"\n",
    "    graph_data_list = []\n",
    "\n",
    "    for current_data, next_data in vtk_pairs:\n",
    "        current_points, current_point_data, _, _ = current_data\n",
    "        next_points, next_point_data, _, next_edge_attributes = next_data\n",
    "\n",
    "        # Interpolate current point data to next points\n",
    "        interpolated_point_data = interpolate_features(current_points, current_point_data, next_points)\n",
    "        interpolated_point_data['theta'] = next_point_data['theta']\n",
    "        \n",
    "        # # Y values are directly the phi values from next points\n",
    "        # y_values = next_point_data['phi']\n",
    "\n",
    "        # Assuming interpolated_point_data and next_point_data are defined and contain 'phi'\n",
    "        y_values = np.round(interpolated_point_data['phi']) - np.round(next_point_data['phi'])\n",
    "        # Keeping only positive values\n",
    "        y_values = np.maximum(y_values, 0)\n",
    "        # Rounding to 1 decimal place\n",
    "        y_values = np.round(y_values, 1)\n",
    "        \n",
    "        # Create the graph data using next points, interpolated data, and attributes\n",
    "        data = create_graph_data(next_points, interpolated_point_data, next_edge_attributes, y_values)\n",
    "        graph_data_list.append(data)\n",
    "\n",
    "        progress_percent = (len(graph_data_list) / len(vtk_pairs)) * 100\n",
    "        print(f\"Processing pair {len(graph_data_list)} of {len(vtk_pairs)} ({progress_percent:.2f}%) completed\")\n",
    "\n",
    "    return graph_data_list\n",
    "\n",
    "graph_data_list = create_graphs_from_datasets(vtk_pairs)\n",
    "\n",
    "# # Assuming `graph_data_list` is already created\n",
    "# torch.save(graph_data_list, './generated_data/graph_data_list.pth')\n",
    "# # Load the graph data list\n",
    "# graph_data_list = torch.load('./generated_data/graph_data_list.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m graph_data\u001b[38;5;241m.\u001b[39medge_attr\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Assuming edge_attr contains vector components for each edge\u001b[39;00m\n\u001b[1;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m graph_data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Assuming y contains target values or another feature for each node\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m myu\u001b[38;5;241m.\u001b[39mplot_graph_components_with_highlights(points, features, edges, edge_attr, y)\n",
      "File \u001b[0;32m/ocean/projects/eng170006p/ussqww/NDD_ML-main/my_utils.py:230\u001b[0m, in \u001b[0;36mplot_graph_components_with_highlights\u001b[0;34m(points, features, edges, edge_attr, y)\u001b[0m\n\u001b[1;32m    228\u001b[0m num_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    229\u001b[0m num_rows \u001b[38;5;241m=\u001b[39m ((num_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m num_columns \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_columns\n\u001b[0;32m--> 230\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(num_rows, num_columns, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m num_rows))\n\u001b[1;32m    231\u001b[0m axs \u001b[38;5;241m=\u001b[39m axs\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Plot node features\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "random_index = random.randint(0, len(graph_data_list) - 1)\n",
    "graph_data = graph_data_list[random_index]\n",
    "points = graph_data.x.numpy()[:, :2]\n",
    "features = graph_data.x.numpy()[:, 2:]  # All features from index 2 to end\n",
    "edges = graph_data.edge_index.numpy().T\n",
    "edge_attr = graph_data.edge_attr.numpy()  # Assuming edge_attr contains vector components for each edge\n",
    "y = graph_data.y.numpy()  # Assuming y contains target values or another feature for each node\n",
    "\n",
    "myu.plot_graph_components_with_highlights(points, features, edges, edge_attr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = myu.combine_graph_data(graph_data_list)\n",
    "# data.x = data.x[:,:3]\n",
    "# Print details about the combined graph data\n",
    "print(\"Combined Graph Data:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.isnan(data.x).any() or torch.isnan(data.y).any():\n",
    "    print(\"NaNs found in the dataset\")\n",
    "if torch.isinf(data.x).any() or torch.isinf(data.y).any():\n",
    "    print(\"Infs found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before removing nodes based on phi values')\n",
    "print(data)\n",
    "data = myu.remove_nodes(data, threshold=0.01)\n",
    "print('After removing nodes based on phi values')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before removing NaN and Inf nodes')\n",
    "print(data)\n",
    "data = myu.remove_nans_and_infs(data)\n",
    "print('After removing NaN and Inf nodes')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y (labels) is of shape [num_nodes]\n",
    "data.y = data.y.view(-1)  # Ensure y is 1D [num_nodes]\n",
    "\n",
    "# Number of nodes in the graph\n",
    "num_nodes = data.x.size(0)  # Infer the number of nodes from x\n",
    "num_train = int(num_nodes * 0.8)  # 80% of nodes for training\n",
    "num_test = num_nodes - num_train  # 20% of nodes for testing\n",
    "\n",
    "# Randomly permute node indices to create random train, validation, and test splits\n",
    "perm = torch.randperm(num_nodes)\n",
    "\n",
    "# Initialize masks for train and test sets\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "# Assign masks based on the random permutation\n",
    "train_mask[perm[:num_train]] = True\n",
    "test_mask[perm[num_train:]] = True\n",
    "\n",
    "# Assign masks to the Data object\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Optional: Create a validation mask from the training set\n",
    "num_val = int(num_train * 0.1)  # 10% of the training set for validation\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask[perm[:num_val]] = True\n",
    "\n",
    "# Assign validation mask to the Data object\n",
    "data.val_mask = val_mask\n",
    "\n",
    "data.num_classes = 2  # Assuming your problem is binary classification\n",
    "\n",
    "data.y = data.y.float()\n",
    "# Print the final structured data object\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.x[:, 2] = np.round(data.x[:, 2], decimals=2)\n",
    "\n",
    "# data.x = data.x[:, :3]\n",
    "\n",
    "# data.y = np.round(data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "data.x = myu.min_max_normalize_features(data.x)\n",
    "data.edge_attr = myu.min_max_normalize_features(data.edge_attr)\n",
    "data.y = myu.min_max_normalize_features(data.y)\n",
    "\n",
    "# # Add gradient features\n",
    "# data = add_gradient_features(data, data.edge_index)\n",
    "# print('Done adding gradient features')\n",
    "\n",
    "# # Optionally, rotate points for augmentation\n",
    "# data = rotate_points(data, 45)  # Rotate by 45 degrees\n",
    "# print('Done rotating points')\n",
    "\n",
    "# # Oversample minority class to balance the dataset\n",
    "# data = oversample_minority_class(data)\n",
    "# print('Done oversampling minority class')\n",
    "\n",
    "# Print data to verify changes\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myu.plot_features_and_target(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example check for NaNs in your data\n",
    "if torch.isnan(data.x).any() or torch.isnan(data.y).any():\n",
    "    print(\"NaNs found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ./generated_data\n",
    "# # Assuming 'data' is your PyTorch Geometric Data object\n",
    "# torch.save(data, './generated_data/NDD.pt')\n",
    "# # Load a single data object\n",
    "# data = torch.load('./generated_data/NDD_per1000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)\n",
    "print(max(data.x[:,2]))\n",
    "print(data.x)\n",
    "print(max(data.y))\n",
    "print(data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myu.print_random_y_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.y.numpy(), bins=50)\n",
    "plt.title('Histogram of Model Output Probabilities')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(data)\n",
    "#     loss = F.mse_loss(out[data.train_mask], data.y[data.train_mask].unsqueeze(1))  # Use MSE loss for continuous output\n",
    "#     loss.backward()\n",
    "    \n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#     optimizer.step()\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test():\n",
    "#     model.eval()\n",
    "#     out = model(data)  # Output shape: [num_nodes, 1]\n",
    "#     out = out.squeeze()  # Squeeze to match target shape [num_nodes]\n",
    "\n",
    "#     train_error = F.mse_loss(out[data.train_mask], data.y[data.train_mask]).item()\n",
    "#     test_error = F.mse_loss(out[data.test_mask], data.y[data.test_mask]).item()\n",
    "    \n",
    "#     return train_error, test_error\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Get the model output\n",
    "    out = model(data)\n",
    "\n",
    "    # Ensure the output is between 0 and 1 by applying a sigmoid, if not already included in the model\n",
    "    out_prob = torch.sigmoid(out[data.train_mask])\n",
    "\n",
    "    # Ensure the target labels are of the correct shape [num_nodes, 1] and type float\n",
    "    target = data.y[data.train_mask].unsqueeze(1).float()\n",
    "\n",
    "    # Use binary cross-entropy loss\n",
    "    loss = F.binary_cross_entropy(out_prob, target)\n",
    "\n",
    "    # Perform backpropagation and an optimization step\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    # Get the model output\n",
    "    out = model(data)\n",
    "\n",
    "    # Ensure the output is between 0 and 1 by applying a sigmoid\n",
    "    out_prob = torch.sigmoid(out)\n",
    "\n",
    "    # Calculate binary cross-entropy loss for train and test sets\n",
    "    train_loss = F.binary_cross_entropy(out_prob[data.train_mask], data.y[data.train_mask].unsqueeze(1).float()).item()\n",
    "    test_loss = F.binary_cross_entropy(out_prob[data.test_mask], data.y[data.test_mask].unsqueeze(1).float()).item()\n",
    "\n",
    "    return train_loss, test_loss\n",
    "\n",
    "# from torch_geometric.data import Batch\n",
    "\n",
    "# def train():\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # Assuming 'devices' and 'data' are properly defined\n",
    "#     data_list = [data.to(device) for device in devices]  # List of data objects for each GPU\n",
    "#     out = model(data_list)  # Model should handle parallel execution and gathering\n",
    "\n",
    "#     # Ensure outputs are gathered to a single device, typically cuda:0\n",
    "#     out_prob = torch.sigmoid(out)  # This should be on a single device after gather\n",
    "#     device = out_prob.device  # Device where the output is located\n",
    "\n",
    "#     # Gather targets to the same device as out_prob\n",
    "#     targets = torch.cat([d.y[d.train_mask].unsqueeze(1).float().to(device) for d in data_list])\n",
    "\n",
    "#     # Compute loss\n",
    "#     loss = F.binary_cross_entropy(out_prob, targets)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test():\n",
    "#     model.eval()\n",
    "\n",
    "#     # Get the model output\n",
    "#     out = model(data)\n",
    "\n",
    "#     # Apply sigmoid activation\n",
    "#     out_prob = torch.sigmoid(out)\n",
    "\n",
    "#     # Calculate binary cross-entropy loss for train and test sets\n",
    "#     train_loss = F.binary_cross_entropy(out_prob[data.train_mask], data.y[data.train_mask].unsqueeze(1).float()).item()\n",
    "#     test_loss = F.binary_cross_entropy(out_prob[data.test_mask], data.y[data.test_mask].unsqueeze(1).float()).item()\n",
    "\n",
    "#     return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "# class ResidualGCNBlock(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, out_channels)\n",
    "#         self.conv2 = GCNConv(out_channels, out_channels)\n",
    "#         if in_channels != out_channels:\n",
    "#             self.projection = torch.nn.Linear(in_channels, out_channels)\n",
    "#         else:\n",
    "#             self.projection = None\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         identity = x\n",
    "#         out = F.relu(self.conv1(x, edge_index))\n",
    "#         out = self.conv2(out, edge_index)\n",
    "        \n",
    "#         if self.projection is not None:\n",
    "#             identity = self.projection(identity)\n",
    "        \n",
    "#         out += identity  # Residual connection\n",
    "#         out = F.relu(out)\n",
    "#         return out\n",
    "\n",
    "# class ResidualGCN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout=0.5):\n",
    "#         super().__init__()\n",
    "#         self.initial_conv = GCNConv(in_channels, hidden_channels)\n",
    "#         self.res_blocks = torch.nn.ModuleList([ResidualGCNBlock(hidden_channels, hidden_channels) for _ in range(num_layers - 1)])\n",
    "#         self.final_conv = GCNConv(hidden_channels, out_channels)\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "#         x = F.relu(self.initial_conv(x, edge_index))\n",
    "#         x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "#         for block in self.res_blocks:\n",
    "#             x = block(x, edge_index)\n",
    "#             x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "#         x = self.final_conv(x, edge_index)\n",
    "#         return x\n",
    "\n",
    "# def train(model, data, optimizer):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(data)  # Shape should be [number of nodes, 1] for regression\n",
    "#     out = out[data.train_mask]  # Applying mask\n",
    "#     targets = data.y[data.train_mask].float()  # Ensure targets are float for MSE loss\n",
    "#     loss = F.mse_loss(out, targets)  # Use MSE loss for continuous output\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss.item()\n",
    "\n",
    "# def accuracy(output, labels, threshold=0.1):\n",
    "#     \"\"\"Calculate accuracy within a given threshold.\"\"\"\n",
    "#     return (output.sub(labels).abs() < threshold).float().mean().item()\n",
    "\n",
    "# def evaluate(model, data):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         out = model(data).squeeze()  # Squeeze to remove extra dimension from output\n",
    "\n",
    "#         train_masked_out = out[data.train_mask]\n",
    "#         test_masked_out = out[data.test_mask]\n",
    "#         train_labels = data.y[data.train_mask].float()\n",
    "#         test_labels = data.y[data.test_mask].float()\n",
    "\n",
    "#         train_acc = accuracy(train_masked_out, train_labels)\n",
    "#         test_acc = accuracy(test_masked_out, test_labels)\n",
    "\n",
    "#         return train_acc, test_acc\n",
    "\n",
    "# # Setup for model training/testing\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = ResidualGCN(data.num_node_features, 64, 1, num_layers=3).to(device)  # Assuming the output is 1 for regression\n",
    "# data = data.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Lists to store training and test accuracy for each epoch\n",
    "# train_acc_list = []\n",
    "# test_acc_list = []\n",
    "\n",
    "# # Example training loop\n",
    "# for epoch in range(1, 101):\n",
    "#     train_loss = train(model, data, optimizer)\n",
    "#     train_acc, test_acc = evaluate(model, data)\n",
    "#     train_acc_list.append(train_acc)\n",
    "#     test_acc_list.append(test_acc)\n",
    "#     print(f'Epoch {epoch}: Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tensor(tensor, name=\"Tensor\"):\n",
    "    if torch.isnan(tensor).any() or torch.isinf(tensor).any():\n",
    "        print(f\"{name} contains NaN or Inf.\")\n",
    "\n",
    "# Example usage in your training loop\n",
    "check_tensor(data.x, \"Input Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.distributed as dist\n",
    "# import torch.multiprocessing as mp\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# def setup(rank, world_size):\n",
    "#     os.environ['MASTER_ADDR'] = 'localhost'\n",
    "#     os.environ['MASTER_PORT'] = '29500'\n",
    "#     dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "#     torch.cuda.set_device(rank)\n",
    "\n",
    "# def cleanup():\n",
    "#     dist.destroy_process_group()\n",
    "\n",
    "# def train(rank, world_size, model, data, optimizer):\n",
    "#     torch.cuda.set_device(rank)\n",
    "#     model.train()\n",
    "\n",
    "#     # Assuming data is distributed and gathered correctly\n",
    "#     data = data.to(rank)\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(data)\n",
    "#     out_prob = torch.sigmoid(out)\n",
    "#     targets = data.y[data.train_mask].unsqueeze(1).float()\n",
    "#     loss = nn.functional.binary_cross_entropy(out_prob, targets)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test(rank, model, data):\n",
    "#     model.eval()\n",
    "#     data = data.to(rank)\n",
    "#     out = model(data)\n",
    "#     out_prob = torch.sigmoid(out)\n",
    "#     loss = nn.functional.binary_cross_entropy(out_prob, data.y[data.test_mask].unsqueeze(1).float())\n",
    "#     return loss.item()\n",
    "\n",
    "# def run(rank, world_size, data):\n",
    "#     setup(rank, world_size)\n",
    "#     model = GATNet(data).to(rank)\n",
    "#     model = DDP(model, device_ids=[rank])\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)\n",
    "\n",
    "#     for epoch in range(1, 201):\n",
    "#         train(rank, world_size, model, data, optimizer)\n",
    "#         if rank == 0:  # Only print from rank 0\n",
    "#             loss = test(rank, model, data)\n",
    "#             print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "#     cleanup()\n",
    "\n",
    "# world_size = torch.cuda.device_count()\n",
    "# mp.spawn(run,\n",
    "#          args=(world_size, data),\n",
    "#          nprocs=world_size,\n",
    "#          join=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and printout device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "print(data)\n",
    "\n",
    "# GPU Memory Monitoring Function\n",
    "def print_gpu_memory(device):\n",
    "    torch.cuda.synchronize(device)\n",
    "    allocated = torch.cuda.memory_allocated(device)\n",
    "    reserved = torch.cuda.memory_reserved(device)\n",
    "    print(f\"Allocated memory: {allocated / (1024**3):.3f} GB\")\n",
    "    print(f\"Reserved memory: {reserved / (1024**3):.3f} GB\")\n",
    "\n",
    "from NeuralNetworks import SplineCNN_SuperPixelNet, Net, GCNNet, GATNet, HybridGAT_SplineNet\n",
    "# Initialize the Net model\n",
    "# model = SplineCNN_SuperPixelNet(data)\n",
    "model = Net(data)  # Instantiate the model with the data\n",
    "# model = GCNNet(data)\n",
    "# model = GATNet(data)\n",
    "# model = HybridGAT_SplineNet(data)\n",
    "\n",
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    if hasattr(m, 'weight') and hasattr(m.weight, 'data'):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "    if hasattr(m, 'bias') and m.bias is not None:\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "# # Apply weights initialization\n",
    "# model.apply(weights_init)\n",
    "\n",
    "# Move to the appropriate device\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)\n",
    "\n",
    "# Lists to store training and test accuracy for each epoch\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    train_acc, test_acc = test()\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Test: {test_acc:.4f}')\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        !nvidia-smi\n",
    "        print_gpu_memory(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the convergence of training and test accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_acc_list) + 1), train_acc_list, label='Train Accuracy')\n",
    "plt.plot(range(1, len(test_acc_list) + 1), test_acc_list, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Convergence Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model_regression(data, model):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    \n",
    "    actual = data.y[data.test_mask]\n",
    "    print(actual.size())\n",
    "    \n",
    "    # # Calculate Mean Squared Error\n",
    "    # mse = F.mse_loss(out[data.test_mask], actual)\n",
    "    # print(f'Test MSE: {mse.item():.4f}')\n",
    "\n",
    "    # Ensure output is within the [0, 1] range using sigmoid (if not already applied in the model)\n",
    "    out_prob = torch.sigmoid(out[data.test_mask]).squeeze()  # Apply sigmoid to ensure the outputs are probabilities.\n",
    "    # Calculate Binary Cross-Entropy\n",
    "    bce = F.binary_cross_entropy(out_prob, actual)\n",
    "    print(f'Test BCE: {bce.item():.4f}')\n",
    "\n",
    "    # Print sample predictions for checking\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    num_samples = min(10, out[data.test_mask].size(0))\n",
    "    for i in range(num_samples):\n",
    "        print(f'Predicted: {out[data.test_mask][i].item():.4f}, Actual: {actual[i].item():.4f}')\n",
    "\n",
    "evaluate_model_regression(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model_regression(data, model):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    \n",
    "    actual = data.y[data.test_mask]\n",
    "    print(actual.size())\n",
    "    \n",
    "    # Ensure output is within the [0, 1] range using sigmoid (if not already applied in the model)\n",
    "    out_prob = torch.sigmoid(out[data.test_mask]).squeeze()  # Apply sigmoid to ensure the outputs are probabilities.\n",
    "    \n",
    "    # Calculate Binary Cross-Entropy\n",
    "    bce = F.binary_cross_entropy(out_prob, actual)\n",
    "    print(f'Test BCE: {bce.item():.4f}')\n",
    "\n",
    "    # Calculate accuracy for the entire test dataset\n",
    "    # predicted_labels = (out_prob > 0.7).int()  # Apply threshold to convert probabilities to binary labels\n",
    "    predicted_labels = out_prob  # Apply threshold to convert probabilities to binary labels\n",
    "    correct_predictions = (predicted_labels == actual).float().sum().item()  # Calculate the number of correct predictions\n",
    "    total_predictions = actual.size(0)  # Total number of predictions\n",
    "    accuracy = correct_predictions / total_predictions  # Calculate accuracy\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Print sample predictions for checking\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    num_samples = min(20, actual.size(0))  # Limit number of samples to print\n",
    "    if num_samples > 0:\n",
    "        sample_indices = torch.randperm(actual.size(0))[:num_samples]  # Randomly pick indices to print\n",
    "        for i in sample_indices:\n",
    "            predicted_label = predicted_labels[i].item()\n",
    "            out_prob_val = out_prob[i].item()\n",
    "            actual_val = actual[i].item()\n",
    "            print(f'Pred label: {predicted_label}, Out prob: {out_prob_val:.4f}, Actual: {actual_val:.0f}')\n",
    "\n",
    "    plt.hist(out_prob.detach().cpu().numpy(), bins=50)\n",
    "    plt.title('Histogram of Model Output Probabilities')\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "evaluate_model_regression(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(data, model, rotation_angle_degrees=-45, num_samples=100):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    \n",
    "    # Get all test indices\n",
    "    test_indices = torch.where(data.test_mask)[0]\n",
    "    \n",
    "    # Randomly sample from the test indices if the number is greater than num_samples\n",
    "    if len(test_indices) > num_samples:\n",
    "        sampled_indices = test_indices[torch.randperm(len(test_indices))[:num_samples]]\n",
    "    else:\n",
    "        sampled_indices = test_indices\n",
    "\n",
    "    # out_prob = torch.sigmoid(out[sampled_indices].squeeze())  # Apply sigmoid to ensure the outputs are probabilities\n",
    "    # threshold = 0.7  # Define the threshold\n",
    "    # pred = (out_prob > threshold).int()  # Convert probabilities to binary labels based on the threshold\n",
    "    pred = torch.sigmoid(out[sampled_indices].squeeze())\n",
    "    actual = data.y[sampled_indices]\n",
    "    input_phi = data.x[sampled_indices, 2]  # Assuming the third column is phi\n",
    "    error = (pred - actual).abs()  # Calculate error as absolute difference\n",
    "\n",
    "    # # Calculate accuracy for the sampled cases\n",
    "    # correct = pred.round().eq(actual.round()).sum().item()\n",
    "    # total = len(sampled_indices)\n",
    "    # accuracy = correct / total\n",
    "    # print(f'Test Accuracy (sampled): {accuracy:.4f} for {num_samples} samples')\n",
    "\n",
    "    # Reverse the rotation for visualization purposes\n",
    "    angle = np.radians(rotation_angle_degrees)\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(angle), np.sin(angle)],\n",
    "        [-np.sin(angle), np.cos(angle)]\n",
    "    ])\n",
    "\n",
    "    coords = data.x[sampled_indices][:, :2].cpu().numpy()  # Ensure the first two columns are coordinates\n",
    "    new_coords = np.dot(coords, rotation_matrix)\n",
    "\n",
    "    # Visualization of 2D predictions\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(28, 7), sharex=True, sharey=True)\n",
    "    scatter = ax[0].scatter(new_coords[:, 0], new_coords[:, 1], c=input_phi.cpu().numpy(), cmap='viridis', s=50)\n",
    "    ax[0].set_title('Input Phi')\n",
    "    ax[0].set_xlabel('X Coordinate')\n",
    "    ax[0].set_ylabel('Y Coordinate')\n",
    "    plt.colorbar(scatter, ax=ax[0], label='Input Phi Value')\n",
    "\n",
    "    scatter = ax[1].scatter(new_coords[:, 0], new_coords[:, 1], c=pred.cpu().numpy(), cmap='viridis', s=50)\n",
    "    ax[1].set_title('Predicted Phi')\n",
    "    ax[1].set_xlabel('X Coordinate')\n",
    "    plt.colorbar(scatter, ax=ax[1], label='Predicted Class')\n",
    "\n",
    "    scatter = ax[2].scatter(new_coords[:, 0], new_coords[:, 1], c=actual.cpu().numpy(), cmap='viridis', s=50)\n",
    "    ax[2].set_title('Actual Phi')\n",
    "    ax[2].set_xlabel('X Coordinate')\n",
    "    plt.colorbar(scatter, ax=ax[2], label='Actual Class')\n",
    "\n",
    "    # Error visualization\n",
    "    scatter = ax[3].scatter(new_coords[:, 0], new_coords[:, 1], c=error.cpu().numpy(), cmap='viridis', s=50)\n",
    "    ax[3].set_title('Absolute error')\n",
    "    ax[3].set_xlabel('X Coordinate')\n",
    "    plt.colorbar(scatter, ax=ax[3], label='Error')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "evaluate_model(data, model, 0, num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # credits:\n",
    "# # how to use DDP module with DDP sampler: https://gist.github.com/sgraaf/5b0caa3a320f28c27c12b5efeb35aa4c\n",
    "# # how to setup a basic DDP example from scratch: https://pytorch.org/tutorials/intermediate/dist_tuto.html\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.distributed as dist\n",
    "# import torch.multiprocessing as mp\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torchvision import datasets, transforms\n",
    "# import random\n",
    "\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "# import math\n",
    "\n",
    "# def get_dataset():\n",
    "#     world_size = dist.get_world_size()\n",
    "#     train_set = datasets.MNIST('./data', train=True, download=True,\n",
    "#                              transform=transforms.Compose([\n",
    "#                                  transforms.ToTensor(),\n",
    "#                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                              ]))\n",
    "#     val_set = datasets.MNIST('./data', train=False, download=True,\n",
    "#                              transform=transforms.Compose([\n",
    "#                                  transforms.ToTensor(),\n",
    "#                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                              ]))\n",
    "    \n",
    "#     train_sampler = DistributedSampler(train_set,num_replicas=world_size)\n",
    "#     val_sampler = DistributedSampler(val_set,num_replicas=world_size)\n",
    "#     batch_size = int(128 / float(world_size))\n",
    "#     print(world_size, batch_size)\n",
    "#     train_loader = DataLoader(\n",
    "#         dataset=train_set,\n",
    "#         sampler=train_sampler,\n",
    "#         batch_size=batch_size\n",
    "#     )\n",
    "#     val_loader = DataLoader(\n",
    "#         dataset=val_set,\n",
    "#         sampler=val_sampler,\n",
    "#         batch_size=batch_size\n",
    "#     )\n",
    "\n",
    "#     return train_loader, val_loader, batch_size\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc2 = nn.Linear(50, 10)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.softmax(x)\n",
    "#         return x\n",
    "# def average_gradients(model):\n",
    "#     size = float(dist.get_world_size())\n",
    "#     for param in model.parameters():\n",
    "#         dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n",
    "#         param.grad.data /= size\n",
    "# def reduce_dict(input_dict, average=True):\n",
    "#     world_size = float(dist.get_world_size())\n",
    "#     names, values = [], []\n",
    "#     for k in sorted(input_dict.keys()):\n",
    "#         names.append(k)\n",
    "#         values.append(input_dict[k])\n",
    "#     values = torch.stack(values, dim=0)\n",
    "#     dist.all_reduce(values, op=dist.ReduceOp.SUM)\n",
    "#     if average:\n",
    "#         values /= world_size\n",
    "#     reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "#     return reduced_dict\n",
    "# def train(model,train_loader,optimizer,batch_size):\n",
    "#     device = torch.device(f\"cuda:{dist.get_rank()}\")\n",
    "#     train_num_batches = int(math.ceil(len(train_loader.dataset) / float(batch_size)))\n",
    "#     model.train()\n",
    "#     # let all processes sync up before starting with a new epoch of training\n",
    "#     # dist.barrier()\n",
    "#     criterion = nn.CrossEntropyLoss().to(device)\n",
    "#     train_loss = 0.0\n",
    "#     for data, target in train_loader:\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = criterion(output, target)\n",
    "#         loss.backward()\n",
    "#         # average gradient as DDP doesn't do it correctly\n",
    "#         average_gradients(model)\n",
    "#         optimizer.step()\n",
    "#         loss_ = {'loss': torch.tensor(loss.item()).to(device)}\n",
    "#         train_loss += reduce_dict(loss_)['loss'].item()\n",
    "#         # cleanup\n",
    "#         # dist.barrier()\n",
    "#         # data, target, output = data.cpu(), target.cpu(), output.cpu()\n",
    "#     train_loss_val = train_loss / train_num_batches\n",
    "#     return train_loss_val\n",
    "# def accuracy(output, target, topk=(1,)):\n",
    "#     \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         maxk = max(topk)\n",
    "#         batch_size = target.size(0)\n",
    "\n",
    "#         _, pred = output.topk(maxk, 1, True, True)\n",
    "#         pred = pred.t()\n",
    "#         correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "#         res = []\n",
    "#         for k in topk:\n",
    "#             correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "#             res.append(correct_k.div_(batch_size))\n",
    "#         return res\n",
    "# def val(model, val_loader,batch_size):\n",
    "#     device = torch.device(f\"cuda:{dist.get_rank()}\")\n",
    "#     val_num_batches = int(math.ceil(len(val_loader.dataset) / float(batch_size)))\n",
    "#     model.eval()\n",
    "#     # let all processes sync up before starting with a new epoch of training\n",
    "#     # dist.barrier()\n",
    "#     criterion = nn.CrossEntropyLoss().to(device)\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in val_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, target)\n",
    "#             loss_ = {'loss': torch.tensor(loss.item()).to(device)}\n",
    "#             val_loss += reduce_dict(loss_)['loss'].item()\n",
    "#     val_loss_val = val_loss / val_num_batches\n",
    "#     return val_loss_val\n",
    "# def run(rank, world_size):\n",
    "#     torch.cuda.set_device(rank)\n",
    "#     torch.cuda.empty_cache()\n",
    "#     device = torch.device(f\"cuda:{rank}\")\n",
    "#     torch.manual_seed(1234)\n",
    "#     train_loader, val_loader, batch_size = get_dataset()\n",
    "#     model = Net().to(device)\n",
    "#     model = nn.SyncBatchNorm.convert_sync_batchnorm(model) # use if model contains batchnorm.\n",
    "#     model = DDP(model,device_ids=[rank],output_device=rank)\n",
    "#     optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.5)\n",
    "#     history =  {\n",
    "#             \"rank\": rank,\n",
    "#             \"train_loss_val\": [],\n",
    "#             \"train_acc_val\": [],\n",
    "#             \"val_loss_val\": [],\n",
    "#             \"val_acc_val\": []\n",
    "#         }\n",
    "#     if rank == 0:\n",
    "#         history = {\n",
    "#             \"rank\": rank,\n",
    "#             \"train_loss_val\": [],\n",
    "#             \"train_acc_val\": [],\n",
    "#             \"val_loss_val\": [],\n",
    "#             \"val_acc_val\": []\n",
    "#         }\n",
    "#     for epoch in range(10):\n",
    "#         train_loss_val = train(model,train_loader,optimizer,batch_size)\n",
    "#         val_loss_val = val(model,val_loader,batch_size)\n",
    "#         if rank == 0:\n",
    "#             print(f'Rank {rank} epoch {epoch}: {train_loss_val:.2f}/{val_loss_val:.2f}')\n",
    "#             history['train_loss_val'].append(train_loss_val)\n",
    "#             history['val_loss_val'].append(val_loss_val)\n",
    "#     print(f'Rank {rank} finished training')\n",
    "#     print(history)\n",
    "#     cleanup(rank)  \n",
    "\n",
    "# def cleanup(rank):\n",
    "#     # dist.cleanup()  \n",
    "#     dist.destroy_process_group()\n",
    "#     print(f\"Rank {rank} is done.\")\n",
    "# def setup_for_distributed(is_master):\n",
    "#     \"\"\"\n",
    "#     This function disables printing when not in master process\n",
    "#     \"\"\"\n",
    "#     import builtins as __builtin__\n",
    "#     builtin_print = __builtin__.print\n",
    "\n",
    "#     def print(*args, **kwargs):\n",
    "#         force = kwargs.pop('force', False)\n",
    "#         if is_master or force:\n",
    "#             builtin_print(*args, **kwargs)\n",
    "\n",
    "#     __builtin__.print = print\n",
    "# def init_process(\n",
    "#         rank, # rank of the process\n",
    "#         world_size, # number of workers\n",
    "#         fn, # function to be run\n",
    "#         # backend='gloo',# good for single node\n",
    "#         # backend='nccl' # the best for CUDA\n",
    "#         backend='gloo'\n",
    "#     ):\n",
    "#     # information used for rank 0\n",
    "#     os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "#     os.environ['MASTER_PORT'] = '29500'\n",
    "#     dist.init_process_group(backend, rank=rank, world_size=world_size)\n",
    "#     dist.barrier()\n",
    "#     setup_for_distributed(rank == 0)\n",
    "#     fn(rank, world_size)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     world_size = 2\n",
    "#     processes = []\n",
    "#     mp.set_start_method(\"spawn\")\n",
    "#     for rank in range(world_size):\n",
    "#         p = mp.Process(target=init_process, args=(rank, world_size, run))\n",
    "#         p.start()\n",
    "#         processes.append(p)\n",
    "\n",
    "#     for p in processes:\n",
    "#         p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg_custom",
   "language": "python",
   "name": "pyg_custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

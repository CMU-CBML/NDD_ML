2024-09-19 13:11:32,296 - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0]
CUDA available: True
CUDA_HOME: /usr/local/cuda
NVCC: 
GPU 0: Tesla V100-SXM2-32GB
GCC: gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-18)
PyTorch: 2.1.0+cu121
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.16.0+cu121
OpenCV: 4.10.0
openstl: 1.0.0
------------------------------------------------------------

2024-09-19 13:11:32,298 - 
device: 	cuda	
dist: 	False	
res_dir: 	work_dirs	
ex_name: 	custom_exp	
fp16: 	False	
torchscript: 	False	
seed: 	42	
fps: 	False	
test: 	False	
deterministic: 	False	
batch_size: 	5	
val_batch_size: 	5	
num_workers: 	4	
data_root: 	./data	
dataname: 	custom	
pre_seq_length: 	10	
aft_seq_length: 	20	
total_length: 	30	
use_augment: 	False	
use_prefetcher: 	False	
drop_last: 	False	
method: 	simvp	
config_file: 	None	
model_type: 	gSTA	
drop: 	0.0	
drop_path: 	0.0	
overwrite: 	False	
epoch: 	100	
log_step: 	1	
opt: 	adam	
opt_eps: 	None	
opt_betas: 	None	
momentum: 	0.9	
weight_decay: 	0.0	
clip_grad: 	None	
clip_mode: 	norm	
no_display_method_info: 	False	
sched: 	onecycle	
lr: 	0.001	
lr_k_decay: 	1.0	
warmup_lr: 	1e-05	
min_lr: 	1e-06	
final_div_factor: 	10000.0	
warmup_epoch: 	0	
decay_epoch: 	100	
decay_rate: 	0.1	
filter_bias_and_bn: 	False	
gpus: 	[0]	
metric_for_bestckpt: 	val_loss	
ckpt_path: 	None	
metrics: 	['mse', 'mae']	
in_shape: 	[10, 1, 32, 32]	
N_S: 	4	
N_T: 	8	
hid_S: 	64	
hid_T: 	256	
2024-09-19 13:11:32,298 - Model info:
SimVP_Model(
  (enc): Encoder(
    (enc): Sequential(
      (0): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
      )
      (1): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
      )
      (2): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
      )
      (3): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
      )
    )
  )
  (dec): Decoder(
    (dec): Sequential(
      (0): ConvSC(
        (conv): BasicConv2d(
          (conv): Sequential(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): PixelShuffle(upscale_factor=2)
          )
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
      )
      (1): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
      )
      (2): ConvSC(
        (conv): BasicConv2d(
          (conv): Sequential(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): PixelShuffle(upscale_factor=2)
          )
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
      )
      (3): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
      )
    )
    (readout): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
  )
  (hid): MidMetaNet(
    (enc): Sequential(
      (0): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(640, 640, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=640)
              (conv_spatial): Conv2d(640, 640, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=640)
              (conv1): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.010)
          (norm2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(640, 5120, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(5120, 5120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5120)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(5120, 640, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (reduction): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
              (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
              (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
              (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
              (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (3): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
              (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
              (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.006)
          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (4): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
              (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
              (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.004)
          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (5): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
              (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
              (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.003)
          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (6): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
              (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
              (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.001)
          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (7): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
              (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
              (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): Identity()
          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (reduction): Conv2d(256, 640, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
| module                           | #parameters or shape   | #flops     |
|:---------------------------------|:-----------------------|:-----------|
| model                            | 18.603M                | 2.333G     |
|  enc.enc                         |  0.112M                |  0.223G    |
|   enc.enc.0.conv                 |   0.768K               |   9.175M   |
|    enc.enc.0.conv.conv           |    0.64K               |    5.898M  |
|    enc.enc.0.conv.norm           |    0.128K              |    3.277M  |
|   enc.enc.1.conv                 |   37.056K              |   95.191M  |
|    enc.enc.1.conv.conv           |    36.928K             |    94.372M |
|    enc.enc.1.conv.norm           |    0.128K              |    0.819M  |
|   enc.enc.2.conv                 |   37.056K              |   95.191M  |
|    enc.enc.2.conv.conv           |    36.928K             |    94.372M |
|    enc.enc.2.conv.norm           |    0.128K              |    0.819M  |
|   enc.enc.3.conv                 |   37.056K              |   23.798M  |
|    enc.enc.3.conv.conv           |    36.928K             |    23.593M |
|    enc.enc.3.conv.norm           |    0.128K              |    0.205M  |
|  dec                             |  0.37M                 |  0.953G    |
|   dec.dec                        |   0.37M                |   0.952G   |
|    dec.dec.0.conv                |    0.148M              |    95.191M |
|    dec.dec.1.conv                |    37.056K             |    95.191M |
|    dec.dec.2.conv                |    0.148M              |    0.381G  |
|    dec.dec.3.conv                |    37.056K             |    0.381G  |
|   dec.readout                    |   65                   |   0.655M   |
|    dec.readout.weight            |    (1, 64, 1, 1)       |            |
|    dec.readout.bias              |    (1,)                |            |
|  hid.enc                         |  18.121M               |  1.157G    |
|   hid.enc.0                      |   8.468M               |   0.541G   |
|    hid.enc.0.block               |    8.304M              |    0.531G  |
|    hid.enc.0.reduction           |    0.164M              |    10.486M |
|   hid.enc.1.block                |   1.356M               |   86.442M  |
|    hid.enc.1.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.1.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.1.block.norm1         |    0.512K              |    81.92K  |
|    hid.enc.1.block.attn          |    0.283M              |    17.99M  |
|    hid.enc.1.block.norm2         |    0.512K              |    81.92K  |
|    hid.enc.1.block.mlp           |    1.071M              |    68.289M |
|   hid.enc.2.block                |   1.356M               |   86.442M  |
|    hid.enc.2.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.2.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.2.block.norm1         |    0.512K              |    81.92K  |
|    hid.enc.2.block.attn          |    0.283M              |    17.99M  |
|    hid.enc.2.block.norm2         |    0.512K              |    81.92K  |
|    hid.enc.2.block.mlp           |    1.071M              |    68.289M |
|   hid.enc.3.block                |   1.356M               |   86.442M  |
|    hid.enc.3.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.3.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.3.block.norm1         |    0.512K              |    81.92K  |
|    hid.enc.3.block.attn          |    0.283M              |    17.99M  |
|    hid.enc.3.block.norm2         |    0.512K              |    81.92K  |
|    hid.enc.3.block.mlp           |    1.071M              |    68.289M |
|   hid.enc.4.block                |   1.356M               |   86.442M  |
|    hid.enc.4.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.4.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.4.block.norm1         |    0.512K              |    81.92K  |
|    hid.enc.4.block.attn          |    0.283M              |    17.99M  |
|    hid.enc.4.block.norm2         |    0.512K              |    81.92K  |
|    hid.enc.4.block.mlp           |    1.071M              |    68.289M |
|   hid.enc.5.block                |   1.356M               |   86.442M  |
|    hid.enc.5.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.5.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.5.block.norm1         |    0.512K              |    81.92K  |
|    hid.enc.5.block.attn          |    0.283M              |    17.99M  |
|    hid.enc.5.block.norm2         |    0.512K              |    81.92K  |
|    hid.enc.5.block.mlp           |    1.071M              |    68.289M |
|   hid.enc.6.block                |   1.356M               |   86.442M  |
|    hid.enc.6.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.6.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.6.block.norm1         |    0.512K              |    81.92K  |
|    hid.enc.6.block.attn          |    0.283M              |    17.99M  |
|    hid.enc.6.block.norm2         |    0.512K              |    81.92K  |
|    hid.enc.6.block.mlp           |    1.071M              |    68.289M |
|   hid.enc.7                      |   1.52M                |   96.928M  |
|    hid.enc.7.block               |    1.356M              |    86.442M |
|    hid.enc.7.reduction           |    0.164M              |    10.486M |
--------------------------------------------------------------------------------

2024-09-19 13:12:45,696 - Epoch 1: Lr: 0.0000505 | Train Loss: 0.0672820 | Vali Loss: 0.0225043
2024-09-19 13:13:01,043 - Epoch 2: Lr: 0.0000636 | Train Loss: 0.0271186 | Vali Loss: 0.0148237
2024-09-19 13:13:14,879 - Epoch 3: Lr: 0.0000817 | Train Loss: 0.0177211 | Vali Loss: 0.0110262
2024-09-19 13:13:27,298 - Epoch 4: Lr: 0.0001046 | Train Loss: 0.0134437 | Vali Loss: 0.0098857
2024-09-19 13:13:39,442 - Epoch 5: Lr: 0.0001321 | Train Loss: 0.0109467 | Vali Loss: 0.0087989
2024-09-19 13:13:52,007 - Epoch 6: Lr: 0.0001639 | Train Loss: 0.0096466 | Vali Loss: 0.0078403
2024-09-19 13:14:04,234 - Epoch 7: Lr: 0.0001996 | Train Loss: 0.0086417 | Vali Loss: 0.0072331
2024-09-19 13:14:16,526 - Epoch 8: Lr: 0.0002388 | Train Loss: 0.0077131 | Vali Loss: 0.0064008
2024-09-19 13:14:29,118 - Epoch 9: Lr: 0.0002811 | Train Loss: 0.0068594 | Vali Loss: 0.0053270
2024-09-19 13:14:42,472 - Epoch 10: Lr: 0.0003261 | Train Loss: 0.0059148 | Vali Loss: 0.0048853
2024-09-19 13:14:54,608 - Epoch 11: Lr: 0.0003731 | Train Loss: 0.0050891 | Vali Loss: 0.0040383
2024-09-19 13:15:06,979 - Epoch 12: Lr: 0.0004218 | Train Loss: 0.0044262 | Vali Loss: 0.0040500
2024-09-19 13:15:18,455 - Epoch 13: Lr: 0.0004716 | Train Loss: 0.0038879 | Vali Loss: 0.0039533
2024-09-19 13:15:30,447 - Epoch 14: Lr: 0.0005219 | Train Loss: 0.0036745 | Vali Loss: 0.0030588
2024-09-19 13:15:44,310 - Epoch 15: Lr: 0.0005722 | Train Loss: 0.0033516 | Vali Loss: 0.0037497
2024-09-19 13:15:55,887 - Epoch 16: Lr: 0.0006219 | Train Loss: 0.0029021 | Vali Loss: 0.0025515
2024-09-19 13:16:08,080 - Epoch 17: Lr: 0.0006705 | Train Loss: 0.0025810 | Vali Loss: 0.0028593
2024-09-19 13:16:19,734 - Epoch 18: Lr: 0.0007175 | Train Loss: 0.0021312 | Vali Loss: 0.0023814
2024-09-19 13:16:31,964 - Epoch 19: Lr: 0.0007622 | Train Loss: 0.0019604 | Vali Loss: 0.0024224
2024-09-19 13:16:43,422 - Epoch 20: Lr: 0.0008043 | Train Loss: 0.0020206 | Vali Loss: 0.0023273
2024-09-19 13:16:55,484 - Epoch 21: Lr: 0.0008433 | Train Loss: 0.0019668 | Vali Loss: 0.0022531
2024-09-19 13:17:07,660 - Epoch 22: Lr: 0.0008787 | Train Loss: 0.0014907 | Vali Loss: 0.0027902
2024-09-19 13:17:20,437 - Epoch 23: Lr: 0.0009101 | Train Loss: 0.0012548 | Vali Loss: 0.0021382
2024-09-19 13:17:44,915 - Epoch 24: Lr: 0.0009373 | Train Loss: 0.0013918 | Vali Loss: 0.0019337
2024-09-19 13:18:01,711 - Epoch 25: Lr: 0.0009599 | Train Loss: 0.0010993 | Vali Loss: 0.0023930
2024-09-19 13:18:17,060 - Epoch 26: Lr: 0.0009776 | Train Loss: 0.0009308 | Vali Loss: 0.0020551
2024-09-19 13:18:32,996 - Epoch 27: Lr: 0.0009902 | Train Loss: 0.0011291 | Vali Loss: 0.0024294
2024-09-19 13:19:14,163 - Epoch 28: Lr: 0.0009977 | Train Loss: 0.0010288 | Vali Loss: 0.0019156
2024-09-19 13:19:29,727 - Epoch 29: Lr: 0.0010000 | Train Loss: 0.0009402 | Vali Loss: 0.0022293
2024-09-19 13:19:55,414 - Epoch 30: Lr: 0.0009994 | Train Loss: 0.0009124 | Vali Loss: 0.0018260
2024-09-19 13:20:37,015 - Epoch 31: Lr: 0.0009978 | Train Loss: 0.0008307 | Vali Loss: 0.0018345
2024-09-19 13:20:55,949 - Epoch 32: Lr: 0.0009952 | Train Loss: 0.0007297 | Vali Loss: 0.0017303
2024-09-19 13:21:41,478 - Epoch 33: Lr: 0.0009917 | Train Loss: 0.0006406 | Vali Loss: 0.0017845
2024-09-19 13:21:58,867 - Epoch 34: Lr: 0.0009871 | Train Loss: 0.0006294 | Vali Loss: 0.0017762
2024-09-19 13:22:11,737 - Epoch 35: Lr: 0.0009815 | Train Loss: 0.0006232 | Vali Loss: 0.0017298
2024-09-19 13:22:44,208 - Epoch 36: Lr: 0.0009750 | Train Loss: 0.0005951 | Vali Loss: 0.0017348
2024-09-19 13:23:19,763 - Epoch 37: Lr: 0.0009675 | Train Loss: 0.0005602 | Vali Loss: 0.0018050
2024-09-19 13:23:56,229 - Epoch 38: Lr: 0.0009591 | Train Loss: 0.0005463 | Vali Loss: 0.0019593
2024-09-19 13:24:24,614 - Epoch 39: Lr: 0.0009497 | Train Loss: 0.0005524 | Vali Loss: 0.0017174
2024-09-19 13:24:47,232 - Epoch 40: Lr: 0.0009395 | Train Loss: 0.0005072 | Vali Loss: 0.0017955
2024-09-19 13:25:00,297 - Epoch 41: Lr: 0.0009283 | Train Loss: 0.0005202 | Vali Loss: 0.0018887
2024-09-19 13:25:13,049 - Epoch 42: Lr: 0.0009163 | Train Loss: 0.0006645 | Vali Loss: 0.0017383
2024-09-19 13:25:30,581 - Epoch 43: Lr: 0.0009035 | Train Loss: 0.0005415 | Vali Loss: 0.0017301
2024-09-19 13:25:53,408 - Epoch 44: Lr: 0.0008898 | Train Loss: 0.0005052 | Vali Loss: 0.0016844
2024-09-19 13:26:09,007 - Epoch 45: Lr: 0.0008754 | Train Loss: 0.0004338 | Vali Loss: 0.0016938
2024-09-19 13:26:55,725 - Epoch 46: Lr: 0.0008602 | Train Loss: 0.0004290 | Vali Loss: 0.0016960
2024-09-19 13:27:13,047 - Epoch 47: Lr: 0.0008443 | Train Loss: 0.0004694 | Vali Loss: 0.0017087
2024-09-19 13:27:25,901 - Epoch 48: Lr: 0.0008277 | Train Loss: 0.0004715 | Vali Loss: 0.0017789
2024-09-19 13:27:38,714 - Epoch 49: Lr: 0.0008104 | Train Loss: 0.0004022 | Vali Loss: 0.0017312
2024-09-19 13:27:51,040 - Epoch 50: Lr: 0.0007925 | Train Loss: 0.0003857 | Vali Loss: 0.0016713
2024-09-19 13:28:04,592 - Epoch 51: Lr: 0.0007740 | Train Loss: 0.0003448 | Vali Loss: 0.0016432
2024-09-19 13:28:21,292 - Epoch 52: Lr: 0.0007550 | Train Loss: 0.0003289 | Vali Loss: 0.0017169
2024-09-19 13:28:33,173 - Epoch 53: Lr: 0.0007354 | Train Loss: 0.0003150 | Vali Loss: 0.0016542
2024-09-19 13:28:46,033 - Epoch 54: Lr: 0.0007154 | Train Loss: 0.0003084 | Vali Loss: 0.0017115
2024-09-19 13:29:22,889 - Epoch 55: Lr: 0.0006949 | Train Loss: 0.0003179 | Vali Loss: 0.0016609
2024-09-19 13:29:55,668 - Epoch 56: Lr: 0.0006741 | Train Loss: 0.0003125 | Vali Loss: 0.0017493
2024-09-19 13:30:12,250 - Epoch 57: Lr: 0.0006529 | Train Loss: 0.0003110 | Vali Loss: 0.0016647
2024-09-19 13:30:31,822 - Epoch 58: Lr: 0.0006314 | Train Loss: 0.0002898 | Vali Loss: 0.0016234
2024-09-19 13:30:55,160 - Epoch 59: Lr: 0.0006096 | Train Loss: 0.0002991 | Vali Loss: 0.0016686
2024-09-19 13:31:07,344 - Epoch 60: Lr: 0.0005876 | Train Loss: 0.0002987 | Vali Loss: 0.0016589
2024-09-19 13:31:22,129 - Epoch 61: Lr: 0.0005654 | Train Loss: 0.0003031 | Vali Loss: 0.0016895
2024-09-19 13:31:40,327 - Epoch 62: Lr: 0.0005431 | Train Loss: 0.0002887 | Vali Loss: 0.0016627
2024-09-19 13:32:21,019 - Epoch 63: Lr: 0.0005207 | Train Loss: 0.0002711 | Vali Loss: 0.0016180
2024-09-19 13:32:42,722 - Epoch 64: Lr: 0.0004983 | Train Loss: 0.0002757 | Vali Loss: 0.0016299
2024-09-19 13:32:56,500 - Epoch 65: Lr: 0.0004758 | Train Loss: 0.0002672 | Vali Loss: 0.0015888
2024-09-19 13:33:10,193 - Epoch 66: Lr: 0.0004535 | Train Loss: 0.0003057 | Vali Loss: 0.0016226
2024-09-19 13:33:22,350 - Epoch 67: Lr: 0.0004312 | Train Loss: 0.0002798 | Vali Loss: 0.0017019
2024-09-19 13:33:42,410 - Epoch 68: Lr: 0.0004090 | Train Loss: 0.0002659 | Vali Loss: 0.0016467
2024-09-19 13:33:58,341 - Epoch 69: Lr: 0.0003871 | Train Loss: 0.0002563 | Vali Loss: 0.0016582
2024-09-19 13:34:15,931 - Epoch 70: Lr: 0.0003653 | Train Loss: 0.0002503 | Vali Loss: 0.0016751
2024-09-19 13:36:58,899 - Epoch 71: Lr: 0.0003439 | Train Loss: 0.0002581 | Vali Loss: 0.0016445
2024-09-19 13:39:28,048 - Epoch 72: Lr: 0.0003227 | Train Loss: 0.0002465 | Vali Loss: 0.0016211
2024-09-19 13:39:42,241 - Epoch 73: Lr: 0.0003019 | Train Loss: 0.0002456 | Vali Loss: 0.0016611
2024-09-19 13:39:53,837 - Epoch 74: Lr: 0.0002815 | Train Loss: 0.0002450 | Vali Loss: 0.0016562
2024-09-19 13:40:05,485 - Epoch 75: Lr: 0.0002615 | Train Loss: 0.0002702 | Vali Loss: 0.0016403
2024-09-19 13:40:17,124 - Epoch 76: Lr: 0.0002421 | Train Loss: 0.0002465 | Vali Loss: 0.0017211
2024-09-19 13:40:28,927 - Epoch 77: Lr: 0.0002231 | Train Loss: 0.0002766 | Vali Loss: 0.0016645
2024-09-19 13:40:41,830 - Epoch 78: Lr: 0.0002047 | Train Loss: 0.0002669 | Vali Loss: 0.0016293
2024-09-19 13:40:53,520 - Epoch 79: Lr: 0.0001869 | Train Loss: 0.0002538 | Vali Loss: 0.0016281
2024-09-19 13:41:06,007 - Epoch 80: Lr: 0.0001697 | Train Loss: 0.0002366 | Vali Loss: 0.0016764
2024-09-19 13:41:17,647 - Epoch 81: Lr: 0.0001532 | Train Loss: 0.0002409 | Vali Loss: 0.0016702
2024-09-19 13:41:31,480 - Epoch 82: Lr: 0.0001374 | Train Loss: 0.0002423 | Vali Loss: 0.0016311
2024-09-19 13:41:43,314 - Epoch 83: Lr: 0.0001223 | Train Loss: 0.0002330 | Vali Loss: 0.0016328
2024-09-19 13:41:57,785 - Epoch 84: Lr: 0.0001080 | Train Loss: 0.0002398 | Vali Loss: 0.0016334
2024-09-19 13:42:10,451 - Epoch 85: Lr: 0.0000945 | Train Loss: 0.0002451 | Vali Loss: 0.0016354
2024-09-19 13:42:23,677 - Epoch 86: Lr: 0.0000818 | Train Loss: 0.0002332 | Vali Loss: 0.0016305
2024-09-19 13:42:36,130 - Epoch 87: Lr: 0.0000699 | Train Loss: 0.0002349 | Vali Loss: 0.0016382
2024-09-19 13:42:48,776 - Epoch 88: Lr: 0.0000589 | Train Loss: 0.0002297 | Vali Loss: 0.0016297
2024-09-19 13:43:01,306 - Epoch 89: Lr: 0.0000488 | Train Loss: 0.0002265 | Vali Loss: 0.0016390
2024-09-19 13:43:14,371 - Epoch 90: Lr: 0.0000396 | Train Loss: 0.0002363 | Vali Loss: 0.0016274
2024-09-19 13:43:26,423 - Epoch 91: Lr: 0.0000313 | Train Loss: 0.0002286 | Vali Loss: 0.0016354
2024-09-19 13:43:38,858 - Epoch 92: Lr: 0.0000239 | Train Loss: 0.0002313 | Vali Loss: 0.0016323
2024-09-19 13:43:51,268 - Epoch 93: Lr: 0.0000176 | Train Loss: 0.0002327 | Vali Loss: 0.0016333
2024-09-19 13:44:07,129 - Epoch 94: Lr: 0.0000122 | Train Loss: 0.0002425 | Vali Loss: 0.0016382
2024-09-19 13:44:21,506 - Epoch 95: Lr: 0.0000077 | Train Loss: 0.0002250 | Vali Loss: 0.0016297
2024-09-19 13:44:33,604 - Epoch 96: Lr: 0.0000043 | Train Loss: 0.0002278 | Vali Loss: 0.0016292
2024-09-19 13:44:45,401 - Epoch 97: Lr: 0.0000019 | Train Loss: 0.0002277 | Vali Loss: 0.0016357
2024-09-19 13:44:57,311 - Epoch 98: Lr: 0.0000004 | Train Loss: 0.0002287 | Vali Loss: 0.0016329
2024-09-19 13:45:08,937 - Epoch 99: Lr: 0.0000000 | Train Loss: 0.0002287 | Vali Loss: 0.0016335
2024-09-19 13:45:16,974 - mse:90.4285659790039, mae:389.0352478027344
2024-09-19 13:45:22,908 - No such comm: 7e0fca1685554d8989f947e92dd2e3cf
2024-09-19 13:45:22,961 - No such comm: 14f5cf3578d84597981845e685a3f61a
2024-09-19 13:45:22,988 - No such comm: f49194fd437f4a47a2bdd97e8887eef8
2024-09-19 13:45:23,013 - No such comm: bc6c3b3190894a398cb455a2a306a899
2024-09-19 13:45:23,027 - No such comm: b742207216f74af79a3b1f6c174a488d
2024-09-19 13:45:23,039 - No such comm: 9c9b81ff8a1f4a21947abb3918c0655d
2024-09-19 13:45:23,051 - No such comm: 6e0aa67959ea4a5fb9ea96c3aa829fc7
2024-09-19 13:45:23,062 - No such comm: b6800db74bca4def882fbbaf2dc5cabd
2024-09-19 13:45:23,073 - No such comm: af0a813afb054dc3a3b9173784133484
2024-09-19 13:45:23,086 - No such comm: 2a5dd07a107b451fb275555e9360dcce
2024-09-19 13:45:23,097 - No such comm: 10d68a0465744af08a4d6a8ec5a94e8f
2024-09-19 13:45:23,108 - No such comm: c6896862cff1469cbf898e5263db48c4
2024-09-19 13:45:23,120 - No such comm: dea3e4e0bb4145609feba7f1a8a3b8de
2024-09-19 13:45:23,131 - No such comm: 4a9e91d5af04435a9fa86eb3543cb797
2024-09-19 13:45:23,143 - No such comm: 63ba6a9cb83947b4958da4e1e87a4ece
2024-09-19 13:45:23,156 - No such comm: d636f98aebda4900bd0b74b12c100048
2024-09-19 13:45:23,168 - No such comm: 250eb0064682459fb7c9820344d113dc
2024-09-19 13:45:23,179 - No such comm: 7e66cb56778940649c738d2879a7a25a
2024-09-19 13:45:23,192 - No such comm: 70cba3f464e94118b84c311b38fbfee0
2024-09-19 13:45:23,203 - No such comm: 962e8d5a7b1842d0ae691cd5b310727f
2024-09-19 13:45:23,216 - No such comm: a6fa4758ecf241ed951af07e5326ab6f
2024-09-19 13:45:23,227 - No such comm: f777329e4abd4f5c8bf0f33f2a2e1820
2024-09-19 13:45:23,238 - No such comm: d9fb1c79e39c4203ac1b7149591fc549
2024-09-19 13:45:23,250 - No such comm: d45ea7eeb0d3416095c7f13b87aff198
2024-09-19 13:45:23,261 - No such comm: 905e729fd4f54338b1d297901c692d4c
2024-09-19 13:45:23,274 - No such comm: 7b32629fc25f4209b1ed1aede0fb558a
2024-09-19 13:45:23,285 - No such comm: 17d3baacb2604046847f527a6c470685
2024-09-19 13:45:23,297 - No such comm: 07b568a6be1749c88046d54af871cdde
2024-09-19 13:45:23,310 - No such comm: b433e855a35c4c5480b092528aaf50b3
2024-09-19 13:45:23,322 - No such comm: b95740ff44e24ab39a32e85bca7be019
2024-09-19 13:45:23,334 - No such comm: 78084874fd1e4f19b5474e2d5f656392
2024-09-19 13:45:23,347 - No such comm: 44edf12b1bd345f8ae39fccd4eaac6ae
2024-09-19 13:45:23,358 - No such comm: 94c3295114ee475d8859970fd3050438
2024-09-19 13:45:23,370 - No such comm: 681dd5f6794f40d892c2b78e6fabbaf8
2024-09-19 13:45:23,382 - No such comm: ddcdd5d0228d45ea98f41c2d4dde43cf
2024-09-19 13:45:23,393 - No such comm: 47544ce729584ac1ad734b4ac6270299
2024-09-19 13:45:23,405 - No such comm: ae1910cada4742b2896397c858271806
2024-09-19 13:45:23,417 - No such comm: 826c6d0848c4466bb242a83bda963b5f
2024-09-19 13:45:23,430 - No such comm: 269d5df96071438b9973402fbfc23718
2024-09-19 13:45:23,441 - No such comm: 4750d1eebe9a421bb9730f11e545c663
2024-09-19 13:45:23,454 - No such comm: 5b92793cd1f1436a9124b0457d18c8e4
2024-09-19 13:45:23,465 - No such comm: 2b40e3005a0f477384d15c8c26ace88d
2024-09-19 13:45:23,477 - No such comm: b5d158daf1c446b0ba6401942bad4d0a
2024-09-19 13:45:23,489 - No such comm: ed0c0dae8b494472b8de2db0dafce261
2024-09-19 13:45:23,500 - No such comm: 11f883470f194d0fa8cd9851e8a1f153
2024-09-19 13:45:23,513 - No such comm: 65d11a6d902640bcb307a405e3c7e516
2024-09-19 13:45:23,525 - No such comm: 1117f1ee08474c838c7427e611880ce0
2024-09-19 13:45:23,537 - No such comm: cd6855d596154900803f90f4c98e4f3a
2024-09-19 13:45:23,549 - No such comm: 2617f0dadb2b4df5be136965843d395e
2024-09-19 13:45:23,562 - No such comm: a4a927e831c44034b45a07c62b94a0c3
2024-09-19 13:45:23,573 - No such comm: ca572adb30da42af9e3ecefaaeb7f3ca
2024-09-19 13:45:23,584 - No such comm: 2bae24fd540247e89dd0459fe0581a02
2024-09-19 13:45:23,596 - No such comm: fecb2c48d6cc4d149e50bdda8f9ad5db
2024-09-19 13:45:23,607 - No such comm: a35ae5f6c8e74af5923e695a167e6425
2024-09-19 13:45:23,618 - No such comm: 10445d49992942ed8315ccaf3afe2df0
2024-09-19 13:45:23,629 - No such comm: e369fceaeb9e452688f09a1db792ac31
2024-09-19 13:45:23,640 - No such comm: 636de7bcd8124b78be5eee6d7abe5fbd
2024-09-19 13:45:23,652 - No such comm: 620fcbaf15cc4ff9ac4735a8d9188b78
2024-09-19 13:45:23,663 - No such comm: a9f876bf5eb24fb0843b12401c3280e5
2024-09-19 13:45:23,674 - No such comm: e0acb6c091e34dbab67c7ce1727393db
2024-09-19 13:45:23,687 - No such comm: 102e61e17dde4d6a922cdbf193acc200
2024-09-19 13:45:23,698 - No such comm: 06b3f923100c40b6b60ff5e93ae8ae4b
2024-09-19 13:45:23,710 - No such comm: 514c5f4044a04e8c9e38c38bb5193385
2024-09-19 13:45:23,722 - No such comm: eee622867f154ad0acb5aea212d3d1ab
2024-09-19 13:45:23,734 - No such comm: 66491ff1e2fd40ad860db6c4ac446ea7
2024-09-19 13:45:23,745 - No such comm: 752613ec1570422980b78f87e66fbaca
2024-09-19 13:45:23,756 - No such comm: e513f257020043aca517afd21c71b6ab
2024-09-19 13:45:23,767 - No such comm: 2f26eccd82d648899e3a0fedc3189d1e
2024-09-19 13:45:23,779 - No such comm: c7559e5db9254eee9af27e8f3689d978
2024-09-19 13:45:23,790 - No such comm: 5588316fc9cb48f69800f635f5c26111
2024-09-19 13:45:23,801 - No such comm: a7d0315178b34910ab044ce66e504708
2024-09-19 13:45:23,814 - No such comm: 68cee7925d6e4d09bd7ba88937fdfc18
2024-09-19 13:45:23,825 - No such comm: 07d3fc3582344631862a227d869a96fe
2024-09-19 13:45:23,838 - No such comm: 91b158dd5e844a4cbdb81c57e209982f
2024-09-19 13:45:23,850 - No such comm: e6fcc8469b2e47a7b0cdc926304edd25
2024-09-19 13:45:23,862 - No such comm: be98d19c584e48acb569a02af1e5e989
2024-09-19 13:45:23,873 - No such comm: 0051d01f00c24421bfe3db9b400fe54a
2024-09-19 13:45:23,884 - No such comm: 87a33d66ba4046f2b0194b65964628d6
2024-09-19 13:45:23,895 - No such comm: 20f1c1b5dc464b1e9038d1b0bd4125cb
2024-09-19 13:45:23,906 - No such comm: 180c3c61972440918b5779562d39f313
2024-09-19 13:45:23,918 - No such comm: eb08780725b74a94a29e5ea85f0e06e7
2024-09-19 13:45:23,930 - No such comm: f316d50854da417cba52c4ef0696aad3
2024-09-19 13:45:23,941 - No such comm: d8560e2b64c34873ae48fbef694412ac
2024-09-19 13:45:23,952 - No such comm: a676e2045a9b479c884fc1ebfb81c361
2024-09-19 13:45:23,964 - No such comm: 98279c3ebaf9419f804640909a320955
2024-09-19 13:45:23,975 - No such comm: 19ca1c59eed34829907ac2de7c13caab
2024-09-19 13:45:23,986 - No such comm: 330c2d477c95412b9d87454763270e54
2024-09-19 13:45:23,997 - No such comm: 9cb361ab9ee3453daaae9a9f9ab6fa62
2024-09-19 13:45:24,009 - No such comm: 8db97bd8bbe84eb4a86b1c7c6919e829
2024-09-19 13:45:24,020 - No such comm: 41a184ac8d0d4c6182f309d6f7ce585e
2024-09-19 13:45:24,032 - No such comm: 68562d0de4884edbaf24ce9885cd988f
2024-09-19 13:45:24,044 - No such comm: 30ef382544b445ff92e2da19434e2cd9
2024-09-19 13:45:24,055 - No such comm: c97eba30b4b4462fa0dbed683ae97d74
2024-09-19 13:45:24,066 - No such comm: a777cd30f7164c7d89bbb2d63f6096b2
2024-09-19 13:45:24,077 - No such comm: aa2842eb0c844d09916d02377f8f37d7
2024-09-19 13:45:24,089 - No such comm: fdcedf964c1c4c8091299b7b53980cf1
2024-09-19 13:45:24,100 - No such comm: 8f6f4ff2391946a48448c2f6df1ad05f
2024-09-19 13:45:24,112 - No such comm: b2a1c662bd2347ffac85becf2731411b
2024-09-19 13:45:24,123 - No such comm: 293ba275920546b39231c9ced51bdf25
2024-09-19 13:45:24,135 - No such comm: 6ac527a2893c4e13994dc044c406bc70
2024-09-19 13:45:24,147 - No such comm: 871e0fc29f79418c99e24864b0b3357a
2024-09-19 14:18:27,039 - mse:90.42855834960938, mae:389.0352478027344
2024-09-19 14:21:23,491 - mse:90.42855072021484, mae:389.0352478027344
2024-09-19 14:29:50,386 - mse:8138.91455078125, mae:9504.369140625

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b64a4-b2ac-48ea-a322-feb01c638b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import gc\n",
    "import my_utils as myu  # Ensure 'myu' is defined with necessary functions\n",
    "\n",
    "ending_iterations = 200000\n",
    "\n",
    "def check_files_for_folder(folder_path, required_early=10, required_later=20, min_later_number=160001, last_later_threshold=ending_iterations):\n",
    "    \"\"\"Check the number of early and later files in the folder and ensure the last later file's number > 200,000.\"\"\"\n",
    "    all_vtk_files = glob.glob(os.path.join(folder_path, 'outputs/*.vtk'))  # Get all VTK files in the folder\n",
    "    file_details = []\n",
    "    messages = []  # List to store messages instead of printing them directly\n",
    "\n",
    "    for file in all_vtk_files:\n",
    "        try:\n",
    "            num = int(file.split('_')[-1].split('.')[0])  # Extract frame number from file name\n",
    "            file_details.append((file, num))\n",
    "        except ValueError:\n",
    "            messages.append(f\"Warning: Unable to parse file number from '{file}'. Skipping this file.\")\n",
    "\n",
    "    file_details_sorted = sorted(file_details, key=lambda x: x[1])  # Sort files based on frame number\n",
    "\n",
    "    # Separate early files (<= 160000) and later files (>= min_later_number)\n",
    "    early_files = [(file, num) for file, num in file_details_sorted if num <= 160000]\n",
    "    later_files = [(file, num) for file, num in file_details_sorted if num >= min_later_number]\n",
    "\n",
    "    # Select a limited number of early files (maximum 10)\n",
    "    if len(early_files) >= required_early:\n",
    "        selected_indices_early = np.linspace(0, len(early_files) - 1, required_early, dtype=int)  # Select required_early evenly spaced files\n",
    "        early_selected = [early_files[i][0] for i in selected_indices_early]  # Select only file paths\n",
    "    else:\n",
    "        messages.append(f\"Warning: Not enough early files in '{folder_path}'. Required: {required_early}, Found: {len(early_files)}.\")\n",
    "        early_selected = [file for file, num in early_files]  # Select all available file paths\n",
    "\n",
    "    # Select a limited number of later files (maximum 20)\n",
    "    if len(later_files) >= required_later:\n",
    "        selected_indices_later = np.linspace(0, len(later_files) - 1, required_later, dtype=int)  # Select required_later evenly spaced files\n",
    "        later_selected = [later_files[i][0] for i in selected_indices_later]  # Select only file paths\n",
    "    elif len(later_files) > 0:\n",
    "        messages.append(f\"Warning: Not enough later files in '{folder_path}'. Required: {required_later}, Found: {len(later_files)}.\")\n",
    "        later_selected = [file for file, num in later_files]  # Select all available file paths\n",
    "    else:\n",
    "        messages.append(f\"Error: No files found with numbers >= {min_later_number} in '{folder_path}'. Skipping this folder.\")\n",
    "        return None, None, None, messages  # Skip the folder if no valid files are found\n",
    "\n",
    "    # Handle the last file in later_files and check if it's corrupted or has an issue\n",
    "    last_file, last_number = None, None\n",
    "    valid_last_file = False\n",
    "\n",
    "    for idx in reversed(range(len(later_files))):  # Loop backwards through the later_files list\n",
    "        try:\n",
    "            # Try to get the file and its number\n",
    "            last_file = later_files[idx][0]\n",
    "            last_number = later_files[idx][1]\n",
    "\n",
    "            # Check if the file has a number > 200,000\n",
    "            if last_number > last_later_threshold:\n",
    "                valid_last_file = True\n",
    "                messages.append(f\"The file '{last_file}' has a valid number > {last_later_threshold}: {last_number}.\")\n",
    "                break  # If the file is valid, exit the loop\n",
    "            else:\n",
    "                messages.append(f\"Warning: The file '{last_file}' has a number <= {last_later_threshold}: {last_number}. Trying previous file.\")\n",
    "\n",
    "        except (IndexError, ValueError, OSError) as e:\n",
    "            # Handle the case where the file is corrupted or unreadable\n",
    "            messages.append(f\"Error: Failed to process file '{last_file}' in '{folder_path}'. Error: {e}. Trying previous file.\")\n",
    "    \n",
    "    # If no valid last file was found, skip the folder\n",
    "    if not valid_last_file:\n",
    "        messages.append(f\"Error: No valid file with a number greater than {last_later_threshold} was found in '{folder_path}'. Skipping this folder.\")\n",
    "        return None, None, None, messages\n",
    "\n",
    "    return early_selected, later_selected, last_number, messages\n",
    "\n",
    "def find_global_bounds(subfolders):\n",
    "    \"\"\"Calculate global bounds based on the 30th VTK file in the later files across all subfolders.\"\"\"\n",
    "    global_min_x, global_min_y = float('inf'), float('inf')  # Initialize min values to infinity\n",
    "    global_max_x, global_max_y = float('-inf'), float('-inf')  # Initialize max values to negative infinity\n",
    "\n",
    "    folder_file_info = {}  # Dictionary to store file information for each folder\n",
    "    problematic_folders = []  # List to collect problematic folders and issues\n",
    "    all_messages = []  # List to collect all messages instead of printing\n",
    "\n",
    "    # Loop through each subfolder to find the global bounds and gather file information\n",
    "    for subfolder in tqdm(subfolders, desc=\"Calculating global bounds\"):\n",
    "        early_files, later_files, last_later_number, messages = check_files_for_folder(subfolder)\n",
    "        all_messages.extend(messages)  # Collect messages from check_files_for_folder\n",
    "\n",
    "        # Skip folder if the last_later_number is smaller than ending_iterations\n",
    "        if last_later_number is not None and last_later_number < ending_iterations:\n",
    "            problematic_folders.append(f\"Skipping folder '{subfolder}' as last_later_number is less than ending_iterations: {last_later_number}.\")\n",
    "            continue\n",
    "\n",
    "        if later_files is None:  # Skip the folder if no valid files are found\n",
    "            problematic_folders.append(f\"Skipping folder '{subfolder}' due to no valid later files.\")\n",
    "            continue\n",
    "\n",
    "        folder_file_info[subfolder] = (early_files, later_files, last_later_number)\n",
    "\n",
    "        # Loop through later_files in reverse to find a valid file for bounds calculation\n",
    "        valid_file_found = False\n",
    "        for idx in reversed(range(len(later_files))):\n",
    "            last_file = later_files[idx]\n",
    "            try:\n",
    "                data = myu.read_mesh_cellCon_exception(last_file, 1, 0)  # Read the mesh and field data\n",
    "                if data:\n",
    "                    points = data[0]  # Extract points data\n",
    "                    # Validate points to ensure they are numeric and finite\n",
    "                    if np.isfinite(points).all():  # Ensure there are no `inf` or `nan` values in points\n",
    "                        min_x, min_y = np.min(points, axis=0)  # Get min x and y values\n",
    "                        max_x, max_y = np.max(points, axis=0)  # Get max x and y values\n",
    "                        # Update the global bounds based on the current file\n",
    "                        global_min_x = min(global_min_x, min_x)\n",
    "                        global_min_y = min(global_min_y, min_y)\n",
    "                        global_max_x = max(global_max_x, max_x)\n",
    "                        global_max_y = max(global_max_y, max_y)\n",
    "                        valid_file_found = True  # A valid file was found\n",
    "                        break  # Exit loop after finding a valid file\n",
    "                    else:\n",
    "                        problematic_folders.append(f\"Error: Invalid (non-finite) points in file '{last_file}'. Skipping.\")\n",
    "                else:\n",
    "                    problematic_folders.append(f\"Error reading mesh data for file '{last_file}' in folder '{subfolder}'. Skipping.\")\n",
    "\n",
    "            except (IndexError, ValueError, OSError) as e:\n",
    "                # Handle the case where the last file is corrupted or unreadable\n",
    "                problematic_folders.append(f\"Error processing file '{last_file}' in folder '{subfolder}'. Error: {e}. Trying previous file.\")\n",
    "\n",
    "        # If no valid file was found, skip the folder\n",
    "        if not valid_file_found:\n",
    "            problematic_folders.append(f\"Error: No valid file found for bounds calculation in '{subfolder}'. Skipping this folder.\")\n",
    "            continue\n",
    "\n",
    "    # Print out problematic folders at the end\n",
    "    if problematic_folders:\n",
    "        all_messages.append(\"\\nProblematic folders encountered:\")\n",
    "        all_messages.extend(problematic_folders)\n",
    "\n",
    "    # Print all messages at once\n",
    "    if all_messages:\n",
    "        print(\"\\n\".join(all_messages))\n",
    "\n",
    "    # Check if bounds are valid (ensure no inf bounds)\n",
    "    if not np.isfinite([global_min_x, global_min_y, global_max_x, global_max_y]).all():\n",
    "        raise ValueError(\"Error: Invalid global bounds detected. Check the input data.\")\n",
    "\n",
    "    # Return the global bounds and the collected file information\n",
    "    return global_min_x, global_min_y, global_max_x, global_max_y, folder_file_info\n",
    "\n",
    "def interpolate_data_for_folder(folder_path, global_bounds, file_info, grid_size=1.0):\n",
    "    \"\"\"Interpolate selected VTK files within a folder using specified global bounds and file information.\"\"\"\n",
    "    global_min_x, global_min_y, global_max_x, global_max_y = global_bounds\n",
    "    x_coords = np.arange(global_min_x, global_max_x + grid_size, grid_size)\n",
    "    y_coords = np.arange(global_min_y, global_max_y + grid_size, grid_size)\n",
    "    grid_x, grid_y = np.meshgrid(x_coords, y_coords)\n",
    "    grid_points = np.vstack([grid_x.ravel(), grid_y.ravel()]).T.astype(np.float32)  # Use float32 to save memory\n",
    "\n",
    "    early_files, later_files, last_later_number = file_info  # Use the pre-collected file information\n",
    "\n",
    "    # Skip folder if the last_later_number is smaller than ending_iterations\n",
    "    if last_later_number is not None and last_later_number < ending_iterations:\n",
    "        print(f\"Skipping folder '{folder_path}' as last_later_number is less than ending_iterations: {last_later_number}.\")\n",
    "        return None, None, None\n",
    "\n",
    "    selected_files = early_files + later_files\n",
    "\n",
    "    if not selected_files:\n",
    "        print(f\"Error: No valid files selected in '{folder_path}'. Skipping this folder.\")\n",
    "        return None, None, None\n",
    "\n",
    "    interpolated_data = []\n",
    "    last_valid_step_data = None\n",
    "\n",
    "    folder_name = os.path.basename(folder_path)  # Get the folder name for display\n",
    "    desc = f'Interpolating in {folder_name}'  # Description for progress bar\n",
    "\n",
    "    # Process each selected file\n",
    "    with tqdm(total=len(selected_files), desc=desc, unit='file') as progress:\n",
    "        for filename in selected_files:\n",
    "            data = myu.read_mesh_cellCon_exception(filename, 1, 0)  # Read the mesh and field data\n",
    "            if data:\n",
    "                points, fields, _, _ = data  # Extract points and scalar fields\n",
    "                interpolated_step_data = {}\n",
    "                for field_name, values in fields.items():\n",
    "                    try:\n",
    "                        # Interpolate each field using the grid points\n",
    "                        interpolated_values = myu.interpolate_features_cKD(points, {field_name: values}, grid_points, k=3)\n",
    "                        interpolated_matrix = interpolated_values[field_name].reshape(len(y_coords), len(x_coords))\n",
    "                        interpolated_step_data[field_name] = interpolated_matrix  # Store the interpolated matrix\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Interpolation failed for field '{field_name}' in file '{filename}': {e}\")\n",
    "                        interpolated_step_data[field_name] = np.full((len(y_coords), len(x_coords)), np.nan)  # Assign NaNs on failure\n",
    "                interpolated_data.append(interpolated_step_data)  # Append interpolated data for this file\n",
    "                last_valid_step_data = interpolated_step_data  # Update last valid data\n",
    "\n",
    "                # Release memory after each file is processed\n",
    "                del points, fields, interpolated_step_data\n",
    "                gc.collect()\n",
    "\n",
    "            else:\n",
    "                if last_valid_step_data is not None:\n",
    "                    print(f\"Info: Using previous valid data for file '{filename}'.\")\n",
    "                    interpolated_data.append(last_valid_step_data.copy())  # Use the last valid data\n",
    "                else:\n",
    "                    print(f\"Warning: No previous valid data to duplicate for file '{filename}'. Assigning NaNs.\")\n",
    "                    # Assign NaNs for all fields if no previous data exists\n",
    "                    duplicated_step_data = {field: np.full((len(y_coords), len(x_coords)), np.nan) for field in myu.REQUIRED_FIELDS}\n",
    "                    interpolated_data.append(duplicated_step_data)\n",
    "\n",
    "            progress.update(1)  # Update progress bar\n",
    "\n",
    "    return interpolated_data, grid_x, grid_y  # Return the interpolated data and grid\n",
    "\n",
    "def process_all_cases(base_path):\n",
    "    \"\"\"Process all cases (subfolders) in the specified base path.\"\"\"\n",
    "    subfolders = glob.glob(os.path.join(base_path, 'io2D_ND*'))  # Get all subfolders matching the pattern\n",
    "\n",
    "    # Sort subfolders based on the numeric part of the folder name (e.g., ND001, ND002, etc.)\n",
    "    subfolders.sort(key=lambda x: int(re.search(r'ND(\\d+)', os.path.basename(x)).group(1)))\n",
    "    if not subfolders:\n",
    "        print(f\"Error: No subfolders found in '{base_path}'.\")\n",
    "        return {}\n",
    "\n",
    "    # Unpack all four global bounds (min_x, min_y, max_x, max_y) and file info\n",
    "    global_min_x, global_min_y, global_max_x, global_max_y, folder_file_info = find_global_bounds(subfolders)\n",
    "    \n",
    "    global_bounds = (global_min_x, global_min_y, global_max_x, global_max_y)  # Pack global bounds into a tuple\n",
    "    print(f\"Global bounds determined: {global_bounds}\")\n",
    "    \n",
    "    cases_data = {}  # Dictionary to store the interpolated data for each case\n",
    "\n",
    "    # Process each subfolder\n",
    "    for subfolder in subfolders:\n",
    "        file_info = folder_file_info.get(subfolder)  # Retrieve file info for this folder\n",
    "        if file_info:  # Only proceed if valid file info is available\n",
    "            interpolated_data, grid_x, grid_y = interpolate_data_for_folder(subfolder, global_bounds, file_info)\n",
    "            if interpolated_data is not None:\n",
    "                case_name = os.path.basename(subfolder)  # Get the case name (subfolder name)\n",
    "                cases_data[case_name] = (interpolated_data, grid_x, grid_y)  # Store the interpolated data\n",
    "            else:\n",
    "                print(f\"Warning: No data interpolated for folder '{subfolder}'.\")\n",
    "        else:\n",
    "            print(f\"Skipping folder '{subfolder}' due to insufficient files.\")\n",
    "\n",
    "    return cases_data  # Return the processed data for all cases\n",
    "\n",
    "# root_folder_path = \"/mnt/e/NeuronGrowth_2024/TTNG_05052024\"\n",
    "# root_folder_path = \"/mnt/e/GenNDD_data\"\n",
    "root_folder_path = \"/ocean/projects/eng170006p/ussqww/GenNDD_data\"\n",
    "\n",
    "all_cases_data = process_all_cases(root_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def downsample_all_cases_data(all_cases_data, target_size=(160, 280)):\n",
    "    \"\"\"\n",
    "    Downsamples each dataset from (241, 241) to (160, 280) for each image and the grid in all_cases_data.\n",
    "\n",
    "    Args:\n",
    "    all_cases_data (dict): Dictionary with keys as case identifiers and values as tuples of\n",
    "                           (interpolated_data, grid_x, grid_y) where interpolated_data is a list of dictionaries\n",
    "                           of shape (#time, #channel, 241, 241).\n",
    "    target_size (tuple): Target downsample size, default (160, 280).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with the same keys and downsampled data as values. Each case will have:\n",
    "          (downsampled_interpolated_data, downsampled_grid_x, downsampled_grid_y).\n",
    "    \"\"\"\n",
    "    downsampled_data = {}\n",
    "\n",
    "    for key, (interpolated_data, grid_x, grid_y) in all_cases_data.items():\n",
    "        time_steps = len(interpolated_data)\n",
    "        channels = interpolated_data[0].keys()  # Assuming all time steps have the same channels\n",
    "        downsampled_tensor = []\n",
    "\n",
    "        # Downsample the grid_x and grid_y to (160, 280)\n",
    "        zoom_factor_grid_x = target_size[0] / grid_x.shape[0]\n",
    "        zoom_factor_grid_y = target_size[1] / grid_y.shape[1]\n",
    "        downsampled_grid_x = zoom(grid_x, zoom_factor_grid_x, order=1)  # Bilinear interpolation\n",
    "        downsampled_grid_y = zoom(grid_y, zoom_factor_grid_y, order=1)\n",
    "\n",
    "        # Downsample the interpolated_data (each time step and channel)\n",
    "        for time_step in interpolated_data:\n",
    "            downsampled_channels = {}\n",
    "            for channel in channels:\n",
    "                image = time_step[channel]  # Get the image for this channel\n",
    "                # Calculate the zoom factor for the image\n",
    "                zoom_factor_image_x = target_size[0] / image.shape[0]\n",
    "                zoom_factor_image_y = target_size[1] / image.shape[1]\n",
    "                # Downsample the image\n",
    "                downsampled_image = zoom(image, (zoom_factor_image_x, zoom_factor_image_y), order=1)  # Bilinear interpolation (order=1)\n",
    "                downsampled_channels[channel] = downsampled_image\n",
    "            downsampled_tensor.append(downsampled_channels)\n",
    "\n",
    "        # Store the downsampled data for this case\n",
    "        downsampled_data[key] = (downsampled_tensor, downsampled_grid_x, downsampled_grid_y)\n",
    "\n",
    "    return downsampled_data\n",
    "\n",
    "# Example usage\n",
    "# Assuming `all_cases_data` is your loaded dataset\n",
    "all_cases_data = downsample_all_cases_data(all_cases_data, target_size=(160, 280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_consistent_dimensions_all_channels(all_cases_data):\n",
    "    \"\"\"\n",
    "    Check if the dimensions of all channels are consistent across all cases in the dataset.\n",
    "\n",
    "    Args:\n",
    "        all_cases_data (dict): Dictionary containing all cases.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if dimensions are consistent for all channels, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a variable to store reference shapes for all channels\n",
    "    reference_shapes = None\n",
    "\n",
    "    for case, case_data in all_cases_data.items():\n",
    "        try:\n",
    "            current_shapes = {channel: case_data[0][0][channel].shape for channel in case_data[0][0]}  # Get shapes for all channels\n",
    "            \n",
    "            # Set the first case's shapes as the reference\n",
    "            if reference_shapes is None:\n",
    "                reference_shapes = current_shapes\n",
    "            # Compare the current shapes with the reference shapes\n",
    "            else:\n",
    "                for channel, current_shape in current_shapes.items():\n",
    "                    if channel in reference_shapes:\n",
    "                        if current_shape != reference_shapes[channel]:\n",
    "                            print(f\"Warning: Shape of '{channel}' in {case} is inconsistent. Expected {reference_shapes[channel]}, but got {current_shape}.\")\n",
    "                            return False\n",
    "                    else:\n",
    "                        print(f\"Warning: Channel '{channel}' found in {case} but missing in the reference case.\")\n",
    "                        return False\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Some channels are missing in {case}.\")\n",
    "            return False\n",
    "\n",
    "    print(f\"All cases have consistent dimensions for all channels.\")\n",
    "    return True\n",
    "\n",
    "def print_all_cases_data_dimensions(all_cases_data):\n",
    "    \"\"\"\n",
    "    Prints the dimensions of the data stored in all_cases_data.\n",
    "\n",
    "    Args:\n",
    "        all_cases_data (dict): Dictionary containing data for all cases. \n",
    "                               Each case is a tuple with (interpolated_data, grid_x, grid_y).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for case_name, (interpolated_data, grid_x, grid_y) in all_cases_data.items():\n",
    "        print(f\"Case: {case_name}\")\n",
    "        # Print dimensions of interpolated_data\n",
    "        if isinstance(interpolated_data, list):\n",
    "            print(f\"  Interpolated Data: {len(interpolated_data)} time steps\")\n",
    "            if len(interpolated_data) > 0 and isinstance(interpolated_data[0], dict):\n",
    "                for channel, data_array in interpolated_data[0].items():\n",
    "                    print(f\"    Channel '{channel}': {data_array.shape}\")\n",
    "        else:\n",
    "            print(f\"  Interpolated Data: {interpolated_data.shape}\")\n",
    "\n",
    "        # Print dimensions of grid_x and grid_y\n",
    "        print(f\"  Grid X: {grid_x.shape}\")\n",
    "        print(f\"  Grid Y: {grid_y.shape}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print_all_cases_data_dimensions(all_cases_data)\n",
    "consistent = check_consistent_dimensions_all_channels(all_cases_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c7cb1-7dae-483d-a64c-c8c2555b57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def plot_interpolated_time_steps(time_step_data, channel, split_at=5, columns_per_row=3):\n",
    "    \"\"\"\n",
    "    Plots the specified channel from each time step's interpolated data, splitting the plots into two figures.\n",
    "\n",
    "    Args:\n",
    "        time_step_data (list of dicts): Each dict contains channel names as keys and 2D arrays of data as values.\n",
    "        channel (str): Name of the channel to plot.\n",
    "        split_at (int): Index at which to split the plots into a second figure.\n",
    "        columns_per_row (int): Number of columns per row in the plot.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plots.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_time_steps = len(time_step_data)\n",
    "    first_batch = min(num_time_steps, split_at)\n",
    "    second_batch = num_time_steps - first_batch\n",
    "\n",
    "    # Function to create plots for a given range of time steps\n",
    "    def plot_batch(start_index, end_index, title):\n",
    "        num_plots = end_index - start_index\n",
    "        rows = (num_plots + columns_per_row - 1) // columns_per_row  # Calculate rows needed\n",
    "        fig, axes = plt.subplots(rows, columns_per_row, figsize=(5 * columns_per_row, 5 * rows))\n",
    "        if not isinstance(axes, np.ndarray):\n",
    "            axes = np.array([axes])  # Handle the case where there is only one subplot\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i in range(num_plots):\n",
    "            ax = axes[i]\n",
    "            data_index = start_index + i\n",
    "            if channel in time_step_data[data_index]:\n",
    "                c = ax.imshow(time_step_data[data_index][channel], cmap='viridis', aspect='auto')\n",
    "                fig.colorbar(c, ax=ax)\n",
    "                ax.set_title(f'Time Step: {data_index + 1}')\n",
    "                ax.set_xlabel('X Coordinate')\n",
    "                ax.set_ylabel('Y Coordinate')\n",
    "            else:\n",
    "                ax.set_title(f'Channel {channel} not found')\n",
    "                ax.set_xlabel('X Coordinate')\n",
    "                ax.set_ylabel('Y Coordinate')\n",
    "                print(f\"Warning: Channel '{channel}' not found in time step {data_index + 1}.\")\n",
    "\n",
    "        for j in range(i + 1, len(axes)):  # Hide unused axes\n",
    "            axes[j].axis('off')\n",
    "\n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    # Plot the first batch of time steps\n",
    "    plot_batch(0, first_batch, \"First Batch of Time Steps\")\n",
    "\n",
    "    # Plot the second batch of time steps if there are any\n",
    "    if second_batch > 0:\n",
    "        plot_batch(first_batch, num_time_steps, \"Second Batch of Time Steps\")\n",
    "\n",
    "# Randomly select a case key from all_cases_data\n",
    "case_key = random.choice(list(all_cases_data.keys()))  # Randomly pick a case key\n",
    "channel_to_plot = 'phi'  # Example channel to plot\n",
    "\n",
    "# Now use the variable to access data and call the plotting function\n",
    "plot_interpolated_time_steps(all_cases_data[case_key][0], channel_to_plot, columns_per_row=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4767234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def select_and_structure_data(all_cases_data, num_X_steps=10, num_Y_steps=20, train_frac=0.7, val_frac=0.15, channels=None):\n",
    "    \"\"\"\n",
    "    Structured data extraction for ML training from multidimensional time-series data.\n",
    "\n",
    "    Args:\n",
    "        all_cases_data (dict): The main dataset containing all cases.\n",
    "        num_X_steps (int): Number of time steps to use for X.\n",
    "        num_Y_steps (int): Number of time steps to use for Y.\n",
    "        train_frac (float): Fraction of data to be used for training.\n",
    "        val_frac (float): Fraction of data to be used for validation.\n",
    "        channels (list): List of channel names to include in the output arrays.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dataset containing structured data for training, validation, and testing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract keys for all cases\n",
    "    case_keys = list(all_cases_data.keys())\n",
    "    random.shuffle(case_keys)\n",
    "\n",
    "    # Determine split indices\n",
    "    num_train = int(len(case_keys) * train_frac)\n",
    "    num_val = int(len(case_keys) * val_frac)\n",
    "    \n",
    "    train_keys = case_keys[:num_train]\n",
    "    val_keys = case_keys[num_train:num_train + num_val]\n",
    "    test_keys = case_keys[num_train + num_val:]\n",
    "\n",
    "    # Define function to extract data for X and Y\n",
    "    def extract_data(cases, num_X_steps, num_Y_steps, channels):\n",
    "        X, Y = [], []\n",
    "        for case in cases:\n",
    "            case_data = all_cases_data[case][0]  # Assuming the data structure is (list of dicts with channel data, grid_x, grid_y)\n",
    "            # print(case_data)\n",
    "            print(case)\n",
    "            case_data[0]['phi']\n",
    "            if len(case_data) >= num_X_steps + num_Y_steps:\n",
    "                x_segments = np.stack([np.array([time_step[channel] for time_step in case_data[:num_X_steps]]) for channel in channels if channel in case_data[0]], axis=1)\n",
    "                y_segments = np.stack([np.array([time_step[channel] for time_step in case_data[num_X_steps:num_X_steps + num_Y_steps]]) for channel in channels if channel in case_data[0]], axis=1)\n",
    "\n",
    "                X.append(x_segments)\n",
    "                Y.append(y_segments)\n",
    "\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "    # Extract data for each dataset\n",
    "    X_train, Y_train = extract_data(train_keys, num_X_steps, num_Y_steps, channels)\n",
    "    X_val, Y_val = extract_data(val_keys, num_X_steps, num_Y_steps, channels)\n",
    "    X_test, Y_test = extract_data(test_keys, num_X_steps, num_Y_steps, channels)\n",
    "\n",
    "    return {\n",
    "        'X_train': X_train, 'Y_train': Y_train,\n",
    "        'X_val': X_val, 'Y_val': Y_val,\n",
    "        'X_test': X_test, 'Y_test': Y_test\n",
    "    }\n",
    "\n",
    "# Example usage: specify which channels to include\n",
    "# channels_to_include = ['phi', 'synaptogenesis', 'tubulin', 'tips', 'theta']  # Add your specific channels here\n",
    "channels_to_include = ['phi']  # Add your specific channels here\n",
    "dataset = select_and_structure_data(all_cases_data, channels=channels_to_include)\n",
    "\n",
    "# Display the shapes of the datasets\n",
    "for key in dataset:\n",
    "    print(f\"{key} shape: {dataset[key].shape if dataset[key].size else 'Empty'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the dataset to a .pkl file using pickle\n",
    "with open('./reformatedNDDs/dataset_16k_20k.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1468fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenSTL",
   "language": "python",
   "name": "openstl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UjoTbUQVnCz8"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade torch-scatter\n",
        "# !pip install --upgrade torch-sparse\n",
        "# !pip install --upgrade torch-cluster\n",
        "# !pip install --upgrade torch-spline-conv\n",
        "# !pip install torch-geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjY9vtO9MgoL"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/rusty1s/pytorch_geometric/master/docs/source/_static/img/pyg_logo_text.svg?sanitize=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3UffAf8M2Gw"
      },
      "source": [
        "# Intorduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4_eVOI2M4Uo"
      },
      "source": [
        "PyTorch Geometric [PyG](https://github.com/rusty1s/pytorch_geometric) is a geometric deep learning (GDN) extension library for PyTorch. In general GDN is used to generalize deep learning for non-Ecludian data. For the most part, CNN doesn't work very good for 3D shapes, point clouds and graph structures. Moreover, many real life datasets are inherently non-ecludian like social communicatin datasets, molecular structures, network traffic . etc ...\n",
        "\n",
        "Graph convolutional networks (GCN) come to the rescue to generalize CNNs to work for non-ecludian datasets. The basic architecture is illustrated below\n",
        "\n",
        "![alt text](https://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png)\n",
        "\n",
        "where the input is a graph $G = (V,E)$ represented as\n",
        "\n",
        "*   Feature repsentation for each node $N \\times D$ where N is the number of nodes in the graph and $D$ is the number of features per node.\n",
        "*   A matrix repsentation of the graph in the form $2\\times L$ where $L$ is the number of edges in the graph. Each column in the matrix represents an edge between two nodes.\n",
        "*  Edge attributes of the form $L \\times R$ where R is the number of features per each edge.\n",
        "\n",
        "The output is of form $N \\times F$ where $F$ is the number of features per each node in the graph.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YeA0slcJnQik"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os.path as osp\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SplineConv\n",
        "from torch_geometric.data import Data\n",
        "from random import shuffle, randint\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pQ-c3ftL_gp"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We will simulate a spammer vs non-spammer graph network. Given a node which represents a client that can send emails to different node (another client).\n",
        "\n",
        "Spammers have some similarities\n",
        "\n",
        "*   More likely to send lots of emails (more edges)\n",
        "*   More likely to send lots of data through email (we will represent an edge feature is the number of bytes where the value [0, 1] where 1 represents more bytes sent)\n",
        "*   Each spammer has an associated trust value which is given by the server. If the node is more likely to be a spammer then the value will be closer to 1.\n",
        "\n",
        "Non-spammers have the opposite features. In the next code snippet will try to simulate all of these features through randomization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MhlVjcdM7l6H"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "N = 100\n",
        "nodes = range(0, N)\n",
        "node_features = []\n",
        "edge_features = []\n",
        "\n",
        "for node in nodes:\n",
        "\n",
        "  #spammer\n",
        "  if random.random() > 0.5:\n",
        "    #more likely to have many connections with a maximum of 1/5 of the nodes in the graph\n",
        "    nb_nbrs = int(random.random() * (N/5))\n",
        "    #more likely to have sent many bytes\n",
        "    node_features.append((random.random()+1) / 2.)\n",
        "    #more likely to have a high trust value\n",
        "    edge_features += [(random.random()+2)/3.] * nb_nbrs\n",
        "    #associate a label\n",
        "    labels.append(1)\n",
        "\n",
        "  #non-spammer\n",
        "  else:\n",
        "    #at most connected to 10 nbrs\n",
        "    nb_nbrs = int(random.random() * 10 + 1)\n",
        "    #associate more bytes and random bytes\n",
        "    node_features.append(random.random())\n",
        "    edge_features += [random.random()] * nb_nbrs\n",
        "    labels.append(0)\n",
        "\n",
        "  #connect to some random nodes\n",
        "  nbrs = np.random.choice(nodes, size = nb_nbrs)\n",
        "  nbrs = nbrs.reshape((1, nb_nbrs))\n",
        "\n",
        "  #add the edges of nbrs\n",
        "  node_edges = np.concatenate([np.ones((1, nb_nbrs), dtype = np.int32) * node, nbrs], axis = 0)\n",
        "\n",
        "  #add the overall edges\n",
        "  if node == 0:\n",
        "    edges = node_edges\n",
        "  else:\n",
        "    edges = np.concatenate([edges, node_edges], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvfuQZv5lcM8"
      },
      "source": [
        "Create a data structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "W1tyghgVFinu",
        "outputId": "2c970876-76f9-4ef7-c8a5-d9a222b0a768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(x=[1000, 1], edge_index=[2, 53625], edge_attr=[53625, 1], y=[1000])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(np.expand_dims(node_features, 1), dtype=torch.float)\n",
        "y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "edge_attr = torch.tensor(np.expand_dims(edge_features, 1), dtype=torch.float)\n",
        "\n",
        "data = Data(x = x, edge_index=edge_index, y =y, edge_attr=edge_attr )\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGcoGWzKlkHy"
      },
      "source": [
        "We will create a trian/test mask where we split the data into training and test. This is necessary because during optimizing the loss when training we don't want to include the nodes part of the testing process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WRwBaYmyoLDX"
      },
      "outputs": [],
      "source": [
        "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
        "data.train_mask[:int(0.8 * data.num_nodes)] = 1 #train only on the 80% nodes\n",
        "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.uint8) #test on 20 % nodes\n",
        "data.test_mask[- int(0.2 * data.num_nodes):] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2YFmL6kl5Dh"
      },
      "source": [
        "# Deep GCN\n",
        "\n",
        "We will use [SplineConv](https://arxiv.org/abs/1711.08920) layer for the convolution. We will illsue exponential ReLU as an activation function and dropout for regulaization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MTlX4IBkoOnm"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = SplineConv(1, 16, dim=1, kernel_size=5)\n",
        "        self.conv2 = SplineConv(16, 32, dim=1, kernel_size=5)\n",
        "        self.conv3 = SplineConv(32, 64, dim=1, kernel_size=7)\n",
        "        self.conv4 = SplineConv(64, 128, dim=1, kernel_size=7)\n",
        "        self.conv5 = SplineConv(128, 128, dim=1, kernel_size=11)\n",
        "        self.conv6 = SplineConv(128, 2, dim=1, kernel_size=11)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        x = F.elu(self.conv1(x, edge_index, edge_attr))\n",
        "        x = self.conv2(x, edge_index, edge_attr)\n",
        "        x = F.elu(self.conv3(x, edge_index, edge_attr))\n",
        "        x = self.conv4(x, edge_index, edge_attr)\n",
        "        x = F.elu(self.conv5(x, edge_index, edge_attr))\n",
        "        x = self.conv6(x, edge_index, edge_attr)\n",
        "        x = F.dropout(x, training = self.training)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pULYL97tmYel"
      },
      "source": [
        "# Optimization\n",
        "\n",
        "We will use nll_loss which can be used for classification of arbitrary classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Hhabp4QvoP6V"
      },
      "outputs": [],
      "source": [
        "def evaluate_loss(mode = 'train'):\n",
        "\n",
        "  #use masking for loss evaluation\n",
        "  if mode == 'train':\n",
        "    loss = F.nll_loss(model()[data.train_mask], data.y[data.train_mask])\n",
        "  else:\n",
        "    loss = F.nll_loss(model()[data.test_mask], data.y[data.test_mask])\n",
        "  return loss\n",
        "\n",
        "def train():\n",
        "  #training\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  loss = evaluate_loss()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss.detach().cpu().numpy()\n",
        "\n",
        "def test():\n",
        "  #testing\n",
        "  model.eval()\n",
        "  logits, accs = model(), []\n",
        "  loss = evaluate_loss(mode = 'test').detach().cpu().numpy()\n",
        "\n",
        "  for _, mask in data('train_mask', 'test_mask'):\n",
        "      pred = logits[mask].max(1)[1]\n",
        "      acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "      accs.append(acc)\n",
        "  return [loss] + accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0XicLqpmqwR"
      },
      "source": [
        "# Setup the model\n",
        "We will create the model and setup training using adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sDvcl5eLoRb3"
      },
      "outputs": [],
      "source": [
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyyfCGZimtX2"
      },
      "source": [
        "# Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3400
        },
        "id": "qsslw_68oS52",
        "outputId": "aa47e6c7-8985-4409-bfe5-805c0e3df3ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20829/2809041810.py:5: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  loss = F.nll_loss(model()[data.train_mask], data.y[data.train_mask])\n",
            "/home/kuanrenqian/miniconda3/envs/SplineCNN/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/tmp/ipykernel_20829/2809041810.py:7: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  loss = F.nll_loss(model()[data.test_mask], data.y[data.test_mask])\n",
            "/tmp/ipykernel_20829/2809041810.py:26: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  pred = logits[mask].max(1)[1]\n",
            "/tmp/ipykernel_20829/2809041810.py:27: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, train_loss: 0.694, test_loss:0.690, train_acc: 0.51, test_acc: 0.48\n",
            "Epoch: 002, train_loss: 0.688, test_loss:0.683, train_acc: 0.51, test_acc: 0.48\n",
            "Epoch: 003, train_loss: 0.682, test_loss:0.673, train_acc: 0.62, test_acc: 0.63\n",
            "Epoch: 004, train_loss: 0.677, test_loss:0.663, train_acc: 0.72, test_acc: 0.75\n",
            "Epoch: 005, train_loss: 0.667, test_loss:0.653, train_acc: 0.72, test_acc: 0.78\n",
            "Epoch: 006, train_loss: 0.661, test_loss:0.642, train_acc: 0.69, test_acc: 0.73\n",
            "Epoch: 007, train_loss: 0.655, test_loss:0.631, train_acc: 0.66, test_acc: 0.70\n",
            "Epoch: 008, train_loss: 0.641, test_loss:0.619, train_acc: 0.66, test_acc: 0.71\n",
            "Epoch: 009, train_loss: 0.641, test_loss:0.605, train_acc: 0.68, test_acc: 0.73\n",
            "Epoch: 010, train_loss: 0.633, test_loss:0.591, train_acc: 0.70, test_acc: 0.74\n",
            "Epoch: 011, train_loss: 0.629, test_loss:0.579, train_acc: 0.72, test_acc: 0.77\n",
            "Epoch: 012, train_loss: 0.612, test_loss:0.566, train_acc: 0.72, test_acc: 0.77\n",
            "Epoch: 013, train_loss: 0.620, test_loss:0.552, train_acc: 0.71, test_acc: 0.76\n",
            "Epoch: 014, train_loss: 0.606, test_loss:0.541, train_acc: 0.69, test_acc: 0.73\n",
            "Epoch: 015, train_loss: 0.595, test_loss:0.533, train_acc: 0.68, test_acc: 0.72\n",
            "Epoch: 016, train_loss: 0.595, test_loss:0.527, train_acc: 0.66, test_acc: 0.71\n",
            "Epoch: 017, train_loss: 0.602, test_loss:0.520, train_acc: 0.68, test_acc: 0.72\n",
            "Epoch: 018, train_loss: 0.597, test_loss:0.515, train_acc: 0.69, test_acc: 0.74\n",
            "Epoch: 019, train_loss: 0.599, test_loss:0.514, train_acc: 0.70, test_acc: 0.74\n",
            "Epoch: 020, train_loss: 0.620, test_loss:0.514, train_acc: 0.70, test_acc: 0.74\n",
            "Epoch: 021, train_loss: 0.602, test_loss:0.514, train_acc: 0.70, test_acc: 0.75\n",
            "Epoch: 022, train_loss: 0.628, test_loss:0.515, train_acc: 0.69, test_acc: 0.74\n",
            "Epoch: 023, train_loss: 0.603, test_loss:0.520, train_acc: 0.67, test_acc: 0.70\n",
            "Epoch: 024, train_loss: 0.596, test_loss:0.524, train_acc: 0.66, test_acc: 0.70\n",
            "Epoch: 025, train_loss: 0.593, test_loss:0.525, train_acc: 0.67, test_acc: 0.70\n",
            "Epoch: 026, train_loss: 0.606, test_loss:0.527, train_acc: 0.69, test_acc: 0.74\n",
            "Epoch: 027, train_loss: 0.595, test_loss:0.530, train_acc: 0.70, test_acc: 0.74\n",
            "Epoch: 028, train_loss: 0.588, test_loss:0.533, train_acc: 0.71, test_acc: 0.74\n",
            "Epoch: 029, train_loss: 0.591, test_loss:0.537, train_acc: 0.72, test_acc: 0.75\n",
            "Epoch: 030, train_loss: 0.590, test_loss:0.539, train_acc: 0.72, test_acc: 0.75\n",
            "Epoch: 031, train_loss: 0.599, test_loss:0.541, train_acc: 0.71, test_acc: 0.76\n",
            "Epoch: 032, train_loss: 0.611, test_loss:0.544, train_acc: 0.71, test_acc: 0.74\n",
            "Epoch: 033, train_loss: 0.593, test_loss:0.546, train_acc: 0.69, test_acc: 0.74\n",
            "Epoch: 034, train_loss: 0.600, test_loss:0.547, train_acc: 0.68, test_acc: 0.72\n",
            "Epoch: 035, train_loss: 0.602, test_loss:0.548, train_acc: 0.68, test_acc: 0.72\n",
            "Epoch: 036, train_loss: 0.597, test_loss:0.546, train_acc: 0.68, test_acc: 0.72\n",
            "Epoch: 037, train_loss: 0.611, test_loss:0.544, train_acc: 0.70, test_acc: 0.72\n",
            "Epoch: 038, train_loss: 0.592, test_loss:0.541, train_acc: 0.71, test_acc: 0.75\n",
            "Epoch: 039, train_loss: 0.603, test_loss:0.538, train_acc: 0.72, test_acc: 0.75\n",
            "Epoch: 040, train_loss: 0.588, test_loss:0.536, train_acc: 0.73, test_acc: 0.76\n",
            "Epoch: 041, train_loss: 0.597, test_loss:0.533, train_acc: 0.73, test_acc: 0.76\n",
            "Epoch: 042, train_loss: 0.585, test_loss:0.531, train_acc: 0.72, test_acc: 0.74\n",
            "Epoch: 043, train_loss: 0.584, test_loss:0.529, train_acc: 0.70, test_acc: 0.74\n",
            "Epoch: 044, train_loss: 0.591, test_loss:0.527, train_acc: 0.70, test_acc: 0.73\n",
            "Epoch: 045, train_loss: 0.586, test_loss:0.525, train_acc: 0.70, test_acc: 0.73\n",
            "Epoch: 046, train_loss: 0.597, test_loss:0.524, train_acc: 0.69, test_acc: 0.72\n",
            "Epoch: 047, train_loss: 0.592, test_loss:0.523, train_acc: 0.70, test_acc: 0.72\n",
            "Epoch: 048, train_loss: 0.602, test_loss:0.523, train_acc: 0.70, test_acc: 0.73\n",
            "Epoch: 049, train_loss: 0.593, test_loss:0.523, train_acc: 0.70, test_acc: 0.72\n",
            "Epoch: 050, train_loss: 0.605, test_loss:0.525, train_acc: 0.69, test_acc: 0.71\n",
            "Epoch: 051, train_loss: 0.598, test_loss:0.528, train_acc: 0.68, test_acc: 0.71\n",
            "Epoch: 052, train_loss: 0.603, test_loss:0.530, train_acc: 0.68, test_acc: 0.70\n",
            "Epoch: 053, train_loss: 0.599, test_loss:0.530, train_acc: 0.69, test_acc: 0.71\n",
            "Epoch: 054, train_loss: 0.587, test_loss:0.530, train_acc: 0.71, test_acc: 0.73\n",
            "Epoch: 055, train_loss: 0.588, test_loss:0.531, train_acc: 0.73, test_acc: 0.74\n",
            "Epoch: 056, train_loss: 0.575, test_loss:0.533, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 057, train_loss: 0.593, test_loss:0.533, train_acc: 0.73, test_acc: 0.77\n",
            "Epoch: 058, train_loss: 0.579, test_loss:0.532, train_acc: 0.73, test_acc: 0.74\n",
            "Epoch: 059, train_loss: 0.601, test_loss:0.533, train_acc: 0.70, test_acc: 0.72\n",
            "Epoch: 060, train_loss: 0.590, test_loss:0.535, train_acc: 0.69, test_acc: 0.71\n",
            "Epoch: 061, train_loss: 0.591, test_loss:0.533, train_acc: 0.70, test_acc: 0.71\n",
            "Epoch: 062, train_loss: 0.581, test_loss:0.529, train_acc: 0.72, test_acc: 0.73\n",
            "Epoch: 063, train_loss: 0.583, test_loss:0.527, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 064, train_loss: 0.594, test_loss:0.525, train_acc: 0.74, test_acc: 0.78\n",
            "Epoch: 065, train_loss: 0.589, test_loss:0.523, train_acc: 0.73, test_acc: 0.75\n",
            "Epoch: 066, train_loss: 0.594, test_loss:0.524, train_acc: 0.71, test_acc: 0.72\n",
            "Epoch: 067, train_loss: 0.592, test_loss:0.525, train_acc: 0.69, test_acc: 0.72\n",
            "Epoch: 068, train_loss: 0.585, test_loss:0.524, train_acc: 0.69, test_acc: 0.71\n",
            "Epoch: 069, train_loss: 0.583, test_loss:0.521, train_acc: 0.71, test_acc: 0.72\n",
            "Epoch: 070, train_loss: 0.595, test_loss:0.519, train_acc: 0.71, test_acc: 0.72\n",
            "Epoch: 071, train_loss: 0.578, test_loss:0.518, train_acc: 0.72, test_acc: 0.74\n",
            "Epoch: 072, train_loss: 0.590, test_loss:0.518, train_acc: 0.72, test_acc: 0.73\n",
            "Epoch: 073, train_loss: 0.583, test_loss:0.518, train_acc: 0.72, test_acc: 0.74\n",
            "Epoch: 074, train_loss: 0.570, test_loss:0.517, train_acc: 0.72, test_acc: 0.73\n",
            "Epoch: 075, train_loss: 0.586, test_loss:0.518, train_acc: 0.71, test_acc: 0.72\n",
            "Epoch: 076, train_loss: 0.576, test_loss:0.520, train_acc: 0.70, test_acc: 0.72\n",
            "Epoch: 077, train_loss: 0.576, test_loss:0.519, train_acc: 0.70, test_acc: 0.72\n",
            "Epoch: 078, train_loss: 0.588, test_loss:0.518, train_acc: 0.71, test_acc: 0.73\n",
            "Epoch: 079, train_loss: 0.589, test_loss:0.517, train_acc: 0.72, test_acc: 0.74\n",
            "Epoch: 080, train_loss: 0.593, test_loss:0.518, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 081, train_loss: 0.581, test_loss:0.518, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 082, train_loss: 0.585, test_loss:0.520, train_acc: 0.72, test_acc: 0.74\n",
            "Epoch: 083, train_loss: 0.577, test_loss:0.522, train_acc: 0.71, test_acc: 0.72\n",
            "Epoch: 084, train_loss: 0.573, test_loss:0.522, train_acc: 0.71, test_acc: 0.72\n",
            "Epoch: 085, train_loss: 0.587, test_loss:0.520, train_acc: 0.72, test_acc: 0.73\n",
            "Epoch: 086, train_loss: 0.576, test_loss:0.518, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 087, train_loss: 0.571, test_loss:0.517, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 088, train_loss: 0.590, test_loss:0.516, train_acc: 0.73, test_acc: 0.74\n",
            "Epoch: 089, train_loss: 0.559, test_loss:0.515, train_acc: 0.72, test_acc: 0.75\n",
            "Epoch: 090, train_loss: 0.579, test_loss:0.515, train_acc: 0.72, test_acc: 0.72\n",
            "Epoch: 091, train_loss: 0.582, test_loss:0.514, train_acc: 0.72, test_acc: 0.72\n",
            "Epoch: 092, train_loss: 0.584, test_loss:0.513, train_acc: 0.72, test_acc: 0.74\n",
            "Epoch: 093, train_loss: 0.570, test_loss:0.512, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 094, train_loss: 0.558, test_loss:0.512, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 095, train_loss: 0.579, test_loss:0.510, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 096, train_loss: 0.584, test_loss:0.510, train_acc: 0.72, test_acc: 0.75\n",
            "Epoch: 097, train_loss: 0.582, test_loss:0.514, train_acc: 0.71, test_acc: 0.72\n",
            "Epoch: 098, train_loss: 0.577, test_loss:0.516, train_acc: 0.71, test_acc: 0.72\n",
            "Epoch: 099, train_loss: 0.578, test_loss:0.513, train_acc: 0.72, test_acc: 0.73\n",
            "Epoch: 100, train_loss: 0.583, test_loss:0.511, train_acc: 0.73, test_acc: 0.77\n",
            "Epoch: 101, train_loss: 0.565, test_loss:0.510, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 102, train_loss: 0.566, test_loss:0.509, train_acc: 0.73, test_acc: 0.77\n",
            "Epoch: 103, train_loss: 0.574, test_loss:0.513, train_acc: 0.71, test_acc: 0.72\n",
            "Epoch: 104, train_loss: 0.561, test_loss:0.514, train_acc: 0.70, test_acc: 0.72\n",
            "Epoch: 105, train_loss: 0.574, test_loss:0.510, train_acc: 0.72, test_acc: 0.72\n",
            "Epoch: 106, train_loss: 0.570, test_loss:0.505, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 107, train_loss: 0.560, test_loss:0.505, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 108, train_loss: 0.558, test_loss:0.503, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 109, train_loss: 0.565, test_loss:0.505, train_acc: 0.73, test_acc: 0.75\n",
            "Epoch: 110, train_loss: 0.571, test_loss:0.511, train_acc: 0.71, test_acc: 0.73\n",
            "Epoch: 111, train_loss: 0.560, test_loss:0.513, train_acc: 0.72, test_acc: 0.73\n",
            "Epoch: 112, train_loss: 0.572, test_loss:0.511, train_acc: 0.73, test_acc: 0.74\n",
            "Epoch: 113, train_loss: 0.560, test_loss:0.508, train_acc: 0.74, test_acc: 0.75\n",
            "Epoch: 114, train_loss: 0.553, test_loss:0.507, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 115, train_loss: 0.572, test_loss:0.503, train_acc: 0.75, test_acc: 0.76\n",
            "Epoch: 116, train_loss: 0.582, test_loss:0.501, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 117, train_loss: 0.561, test_loss:0.502, train_acc: 0.73, test_acc: 0.75\n",
            "Epoch: 118, train_loss: 0.567, test_loss:0.500, train_acc: 0.73, test_acc: 0.75\n",
            "Epoch: 119, train_loss: 0.552, test_loss:0.494, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 120, train_loss: 0.556, test_loss:0.493, train_acc: 0.75, test_acc: 0.77\n",
            "Epoch: 121, train_loss: 0.551, test_loss:0.493, train_acc: 0.74, test_acc: 0.78\n",
            "Epoch: 122, train_loss: 0.553, test_loss:0.494, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 123, train_loss: 0.563, test_loss:0.500, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 124, train_loss: 0.552, test_loss:0.505, train_acc: 0.74, test_acc: 0.75\n",
            "Epoch: 125, train_loss: 0.553, test_loss:0.503, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 126, train_loss: 0.558, test_loss:0.500, train_acc: 0.75, test_acc: 0.77\n",
            "Epoch: 127, train_loss: 0.555, test_loss:0.496, train_acc: 0.75, test_acc: 0.78\n",
            "Epoch: 128, train_loss: 0.553, test_loss:0.491, train_acc: 0.75, test_acc: 0.77\n",
            "Epoch: 129, train_loss: 0.541, test_loss:0.486, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 130, train_loss: 0.558, test_loss:0.486, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 131, train_loss: 0.552, test_loss:0.489, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 132, train_loss: 0.546, test_loss:0.490, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 133, train_loss: 0.544, test_loss:0.493, train_acc: 0.75, test_acc: 0.77\n",
            "Epoch: 134, train_loss: 0.552, test_loss:0.495, train_acc: 0.75, test_acc: 0.76\n",
            "Epoch: 135, train_loss: 0.554, test_loss:0.494, train_acc: 0.75, test_acc: 0.76\n",
            "Epoch: 136, train_loss: 0.543, test_loss:0.491, train_acc: 0.75, test_acc: 0.76\n",
            "Epoch: 137, train_loss: 0.543, test_loss:0.485, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 138, train_loss: 0.545, test_loss:0.480, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 139, train_loss: 0.532, test_loss:0.479, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 140, train_loss: 0.552, test_loss:0.484, train_acc: 0.75, test_acc: 0.74\n",
            "Epoch: 141, train_loss: 0.535, test_loss:0.488, train_acc: 0.76, test_acc: 0.74\n",
            "Epoch: 142, train_loss: 0.545, test_loss:0.492, train_acc: 0.75, test_acc: 0.74\n",
            "Epoch: 143, train_loss: 0.556, test_loss:0.494, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 144, train_loss: 0.547, test_loss:0.493, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 145, train_loss: 0.541, test_loss:0.480, train_acc: 0.76, test_acc: 0.76\n",
            "Epoch: 146, train_loss: 0.543, test_loss:0.477, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 147, train_loss: 0.530, test_loss:0.476, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 148, train_loss: 0.547, test_loss:0.478, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 149, train_loss: 0.536, test_loss:0.482, train_acc: 0.74, test_acc: 0.74\n",
            "Epoch: 150, train_loss: 0.539, test_loss:0.489, train_acc: 0.75, test_acc: 0.72\n",
            "Epoch: 151, train_loss: 0.531, test_loss:0.485, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 152, train_loss: 0.524, test_loss:0.481, train_acc: 0.75, test_acc: 0.75\n",
            "Epoch: 153, train_loss: 0.541, test_loss:0.485, train_acc: 0.75, test_acc: 0.74\n",
            "Epoch: 154, train_loss: 0.533, test_loss:0.488, train_acc: 0.75, test_acc: 0.72\n",
            "Epoch: 155, train_loss: 0.532, test_loss:0.484, train_acc: 0.74, test_acc: 0.75\n",
            "Epoch: 156, train_loss: 0.527, test_loss:0.480, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 157, train_loss: 0.528, test_loss:0.482, train_acc: 0.75, test_acc: 0.74\n",
            "Epoch: 158, train_loss: 0.535, test_loss:0.485, train_acc: 0.75, test_acc: 0.74\n",
            "Epoch: 159, train_loss: 0.542, test_loss:0.494, train_acc: 0.75, test_acc: 0.71\n",
            "Epoch: 160, train_loss: 0.532, test_loss:0.490, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 161, train_loss: 0.543, test_loss:0.483, train_acc: 0.75, test_acc: 0.75\n",
            "Epoch: 162, train_loss: 0.539, test_loss:0.482, train_acc: 0.74, test_acc: 0.77\n",
            "Epoch: 163, train_loss: 0.538, test_loss:0.488, train_acc: 0.74, test_acc: 0.74\n",
            "Epoch: 164, train_loss: 0.550, test_loss:0.490, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 165, train_loss: 0.522, test_loss:0.484, train_acc: 0.75, test_acc: 0.75\n",
            "Epoch: 166, train_loss: 0.533, test_loss:0.486, train_acc: 0.75, test_acc: 0.74\n",
            "Epoch: 167, train_loss: 0.541, test_loss:0.490, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 168, train_loss: 0.538, test_loss:0.487, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 169, train_loss: 0.537, test_loss:0.484, train_acc: 0.74, test_acc: 0.74\n",
            "Epoch: 170, train_loss: 0.538, test_loss:0.480, train_acc: 0.74, test_acc: 0.76\n",
            "Epoch: 171, train_loss: 0.548, test_loss:0.483, train_acc: 0.74, test_acc: 0.74\n",
            "Epoch: 172, train_loss: 0.535, test_loss:0.486, train_acc: 0.76, test_acc: 0.73\n",
            "Epoch: 173, train_loss: 0.530, test_loss:0.485, train_acc: 0.76, test_acc: 0.74\n",
            "Epoch: 174, train_loss: 0.532, test_loss:0.484, train_acc: 0.76, test_acc: 0.74\n",
            "Epoch: 175, train_loss: 0.538, test_loss:0.487, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 176, train_loss: 0.527, test_loss:0.483, train_acc: 0.75, test_acc: 0.75\n",
            "Epoch: 177, train_loss: 0.527, test_loss:0.477, train_acc: 0.75, test_acc: 0.76\n",
            "Epoch: 178, train_loss: 0.544, test_loss:0.478, train_acc: 0.75, test_acc: 0.76\n",
            "Epoch: 179, train_loss: 0.546, test_loss:0.485, train_acc: 0.75, test_acc: 0.74\n",
            "Epoch: 180, train_loss: 0.555, test_loss:0.502, train_acc: 0.73, test_acc: 0.69\n",
            "Epoch: 181, train_loss: 0.545, test_loss:0.486, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 182, train_loss: 0.536, test_loss:0.477, train_acc: 0.76, test_acc: 0.76\n",
            "Epoch: 183, train_loss: 0.527, test_loss:0.477, train_acc: 0.75, test_acc: 0.76\n",
            "Epoch: 184, train_loss: 0.535, test_loss:0.488, train_acc: 0.75, test_acc: 0.74\n",
            "Epoch: 185, train_loss: 0.527, test_loss:0.491, train_acc: 0.74, test_acc: 0.72\n",
            "Epoch: 186, train_loss: 0.537, test_loss:0.488, train_acc: 0.76, test_acc: 0.73\n",
            "Epoch: 187, train_loss: 0.527, test_loss:0.486, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 188, train_loss: 0.538, test_loss:0.481, train_acc: 0.75, test_acc: 0.76\n",
            "Epoch: 189, train_loss: 0.541, test_loss:0.482, train_acc: 0.75, test_acc: 0.75\n",
            "Epoch: 190, train_loss: 0.528, test_loss:0.482, train_acc: 0.76, test_acc: 0.75\n",
            "Epoch: 191, train_loss: 0.534, test_loss:0.486, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 192, train_loss: 0.537, test_loss:0.492, train_acc: 0.75, test_acc: 0.72\n",
            "Epoch: 193, train_loss: 0.539, test_loss:0.491, train_acc: 0.75, test_acc: 0.72\n",
            "Epoch: 194, train_loss: 0.533, test_loss:0.483, train_acc: 0.75, test_acc: 0.74\n",
            "Epoch: 195, train_loss: 0.538, test_loss:0.481, train_acc: 0.76, test_acc: 0.75\n",
            "Epoch: 196, train_loss: 0.534, test_loss:0.482, train_acc: 0.75, test_acc: 0.74\n",
            "Epoch: 197, train_loss: 0.533, test_loss:0.486, train_acc: 0.75, test_acc: 0.73\n",
            "Epoch: 198, train_loss: 0.534, test_loss:0.487, train_acc: 0.76, test_acc: 0.73\n",
            "Epoch: 199, train_loss: 0.532, test_loss:0.485, train_acc: 0.76, test_acc: 0.73\n"
          ]
        }
      ],
      "source": [
        "losses = []\n",
        "for epoch in range(1, 200):\n",
        "  train_loss = train()\n",
        "  log = 'Epoch: {:03d}, train_loss: {:.3f}, test_loss:{:.3f}, train_acc: {:.2f}, test_acc: {:.2f}'\n",
        "  test_loss = test()[0]\n",
        "  losses.append([train_loss,test_loss])\n",
        "  print(log.format(epoch, train_loss, *test()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adWu02_enxNp"
      },
      "source": [
        "# References\n",
        "[1] https://github.com/rusty1s/pytorch_geometric\n",
        "\n",
        "[2] https://rusty1s.github.io/pytorch_geometric/build/html/notes/introduction.html\n",
        "\n",
        "[3] https://tkipf.github.io/graph-convolutional-networks/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep GCN Spam.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
